"""
å¾®è°ƒè®­ç»ƒå™¨

ä¸“é—¨ç”¨äºå¾®è°ƒUB-Diffæ¨¡å‹çš„åœ°éœ‡è§£ç å™¨éƒ¨åˆ†
"""

import os
import time
import datetime
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
from typing import Dict, Any, Optional, Tuple
from .pytorch_ssim import SSIM

from ..ub_diff import UBDiff
from ..data import create_dataloaders
from .utils import (
    MetricLogger, SmoothedValue, WarmupMultiStepLR, 
    setup_seed, save_checkpoint, load_checkpoint, count_parameters
)

try:
    import wandb
    _has_wandb = True
except ImportError:
    _has_wandb = False


class FinetuneTrainer:
    """å¾®è°ƒè®­ç»ƒå™¨
    
    ä¸“é—¨ç”¨äºå¾®è°ƒåœ°éœ‡è§£ç å™¨ï¼Œåœ¨é¢„è®­ç»ƒçš„ç¼–ç å™¨åŸºç¡€ä¸Šè®­ç»ƒåœ°éœ‡æ•°æ®ç”Ÿæˆ
    """
    
    def __init__(self,
                 seismic_folder: str,
                 velocity_folder: str,
                 dataset_name: str,
                 output_path: str,
                 checkpoint_path: str,
                 num_data: int = 24000,
                 paired_num: int = 5000,
                 batch_size: int = 64,
                 learning_rate: float = 1e-4,
                 weight_decay: float = 1e-4,
                 lr_gamma: float = 0.98,
                 lr_milestones: Optional[list] = None,
                 warmup_epochs: int = 0,
                 num_workers: int = 4,
                 encoder_dim: int = 512,
                 lambda_g1v: float = 1.0,
                 lambda_g2v: float = 1.0,
                 use_wandb: bool = False,
                 wandb_project: str = "UB-Diff",
                 device: str = "cuda",
                 preload: bool = True,
                 preload_workers: int = 8,
                 cache_size: int = 32,
                 use_memmap: bool = False):
        """
        Args:
            seismic_folder: åœ°éœ‡æ•°æ®æ–‡ä»¶å¤¹è·¯å¾„
            velocity_folder: é€Ÿåº¦åœºæ•°æ®æ–‡ä»¶å¤¹è·¯å¾„
            dataset_name: æ•°æ®é›†åç§°
            output_path: è¾“å‡ºè·¯å¾„
            checkpoint_path: é¢„è®­ç»ƒæ¨¡å‹æ£€æŸ¥ç‚¹è·¯å¾„
            num_data: è®­ç»ƒæ•°æ®æ•°é‡
            paired_num: é…å¯¹æ•°æ®æ•°é‡
            batch_size: æ‰¹æ¬¡å¤§å°
            learning_rate: å­¦ä¹ ç‡
            weight_decay: æƒé‡è¡°å‡
            lr_gamma: å­¦ä¹ ç‡è¡°å‡å› å­
            lr_milestones: å­¦ä¹ ç‡è¡°å‡é‡Œç¨‹ç¢‘
            warmup_epochs: é¢„çƒ­è½®æ•°
            num_workers: æ•°æ®åŠ è½½çº¿ç¨‹æ•°
            encoder_dim: ç¼–ç å™¨ç»´åº¦
            lambda_g1v: L1æŸå¤±æƒé‡
            lambda_g2v: L2æŸå¤±æƒé‡
            use_wandb: æ˜¯å¦ä½¿ç”¨wandbè®°å½•
            wandb_project: wandbé¡¹ç›®åç§°
            device: è®­ç»ƒè®¾å¤‡
            preload: æ˜¯å¦é¢„åŠ è½½æ•°æ®
            preload_workers: é¢„åŠ è½½ä½¿ç”¨çš„çº¿ç¨‹æ•°
            cache_size: LRUç¼“å­˜å¤§å°
            use_memmap: æ˜¯å¦ä½¿ç”¨å†…å­˜æ˜ å°„
        """
        self.device = torch.device(device)
        self.output_path = output_path
        self.use_wandb = use_wandb and _has_wandb
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        os.makedirs(output_path, exist_ok=True)
        
        # åŠ è½½æ•°æ®ï¼ˆä¼˜åŒ–ç‰ˆæœ¬ï¼‰
        self.train_loader, self.test_loader, self.paired_loader, self.dataset_ctx = create_dataloaders(
            seismic_folder=seismic_folder,
            velocity_folder=velocity_folder,
            dataset_name=dataset_name,
            num_data=num_data,
            paired_num=paired_num,
            batch_size=batch_size,
            num_workers=num_workers,
            preload=preload,
            preload_workers=preload_workers,
            cache_size=cache_size,
            use_memmap=use_memmap,
            prefetch_factor=4,  # å¢åŠ é¢„å–å› å­ä»¥å‡å°‘IOç­‰å¾…
            persistent_workers=True  # ä½¿ç”¨æŒä¹…workerå‡å°‘åˆå§‹åŒ–å¼€é”€
        )
        
        # åˆ›å»ºæ¨¡å‹å¹¶åŠ è½½é¢„è®­ç»ƒæƒé‡
        self.model = UBDiff(
            in_channels=1,
            encoder_dim=encoder_dim,
            velocity_channels=1,
            seismic_channels=5,
            pretrained_path=checkpoint_path
        ).to(self.device)
        
        # å†»ç»“ç¼–ç å™¨å’Œé€Ÿåº¦è§£ç å™¨ï¼Œåªè®­ç»ƒåœ°éœ‡è§£ç å™¨
        self.model.freeze_encoder()
        self.model.freeze_velocity_decoder()
        
        # å†»ç»“æ‰©æ•£éƒ¨åˆ†
        for param in self.model.unet.parameters():
            param.requires_grad = False
        for param in self.model.diffusion.parameters():
            param.requires_grad = False
        
        # æŸå¤±å‡½æ•°
        self.l1_loss = nn.L1Loss()
        self.l2_loss = nn.MSELoss()
        self.lambda_g1v = lambda_g1v
        self.lambda_g2v = lambda_g2v
        
        # ä¼˜åŒ–å™¨å’Œè°ƒåº¦å™¨ï¼ˆåªä¼˜åŒ–åœ°éœ‡è§£ç å™¨å‚æ•°ï¼‰
        trainable_params = [p for p in self.model.parameters() if p.requires_grad]
        
        self.optimizer = torch.optim.AdamW(
            trainable_params,
            lr=learning_rate,
            weight_decay=weight_decay
        )
        
        # å­¦ä¹ ç‡è°ƒåº¦
        if lr_milestones is None:
            lr_milestones = []
        
        warmup_iters = warmup_epochs * len(self.paired_loader)
        lr_milestones_iter = [len(self.paired_loader) * m for m in lr_milestones]
        
        self.scheduler = WarmupMultiStepLR(
            self.optimizer,
            milestones=lr_milestones_iter,
            gamma=lr_gamma,
            warmup_iters=warmup_iters,
            warmup_factor=1e-5
        )
        
        # è®­ç»ƒçŠ¶æ€
        self.step = 0
        self.best_ssim = 0.0
        self.best_loss = float('inf')
        
        # åˆå§‹åŒ–wandb
        if self.use_wandb:
            wandb.init(project=wandb_project, name=f"finetune_{dataset_name}")
        
        print(f"å¾®è°ƒè®­ç»ƒå™¨åˆå§‹åŒ–å®Œæˆ")
        print(f"æ¨¡å‹å‚æ•°ç»Ÿè®¡: {count_parameters(self.model)}")
        print(f"å¯è®­ç»ƒå‚æ•°ç»Ÿè®¡: {sum(p.numel() for p in trainable_params)}")
        
        # è¯¦ç»†å‚æ•°å†»ç»“çŠ¶æ€æ£€æŸ¥
        self._verify_parameter_freezing()

    def _verify_parameter_freezing(self):
        """éªŒè¯å‚æ•°å†»ç»“çŠ¶æ€"""
        print("\n" + "="*50)
        print("å‚æ•°å†»ç»“çŠ¶æ€éªŒè¯")
        print("="*50)
        
        # ç»Ÿè®¡å„ç»„ä»¶çš„å¯è®­ç»ƒå‚æ•°
        encoder_trainable = 0
        velocity_decoder_trainable = 0
        seismic_decoder_trainable = 0
        other_trainable = 0
        
        problematic_params = []
        
        for name, param in self.model.named_parameters():
            if param.requires_grad:
                param_count = param.numel()
                
                if 'encoder' in name:
                    encoder_trainable += param_count
                    problematic_params.append(f"ç¼–ç å™¨å‚æ•°: {name}")
                elif 'velocity' in name and ('decoder' in name or 'projector' in name):
                    velocity_decoder_trainable += param_count
                    problematic_params.append(f"é€Ÿåº¦è§£ç å™¨å‚æ•°: {name}")
                elif 'seismic' in name and ('decoder' in name or 'projector' in name):
                    seismic_decoder_trainable += param_count
                else:
                    other_trainable += param_count
        
        print(f"ç¼–ç å™¨å¯è®­ç»ƒå‚æ•°: {encoder_trainable:,}")
        print(f"é€Ÿåº¦è§£ç å™¨å¯è®­ç»ƒå‚æ•°: {velocity_decoder_trainable:,}")
        print(f"åœ°éœ‡è§£ç å™¨å¯è®­ç»ƒå‚æ•°: {seismic_decoder_trainable:,}")
        print(f"å…¶ä»–å¯è®­ç»ƒå‚æ•°: {other_trainable:,}")
        
        # æ£€æŸ¥æ˜¯å¦æœ‰é—®é¢˜
        if encoder_trainable > 0 or velocity_decoder_trainable > 0:
            print("\nâŒ æ£€æµ‹åˆ°å‚æ•°å†»ç»“é—®é¢˜!")
            for param_name in problematic_params:
                if 'encoder' in param_name or 'velocity' in param_name:
                    print(f"  {param_name}")
            
            # å¼ºåˆ¶é‡æ–°å†»ç»“
            print("\nğŸ”§ å¼ºåˆ¶é‡æ–°å†»ç»“å‚æ•°...")
            self._force_freeze_parameters()
        else:
            print("\nâœ… å‚æ•°å†»ç»“çŠ¶æ€æ­£ç¡®")

    def _force_freeze_parameters(self):
        """å¼ºåˆ¶å†»ç»“åº”è¯¥è¢«å†»ç»“çš„å‚æ•°"""
        # å¼ºåˆ¶å†»ç»“ç¼–ç å™¨
        for param in self.model.encoder.parameters():
            param.requires_grad = False
        
        # å¼ºåˆ¶å†»ç»“é€Ÿåº¦è§£ç å™¨å’ŒæŠ•å½±å™¨
        for param in self.model.dual_decoder.velocity_decoder.parameters():
            param.requires_grad = False
        for param in self.model.dual_decoder.velocity_projector.parameters():
            param.requires_grad = False
        
        # å¼ºåˆ¶å†»ç»“æ‰©æ•£éƒ¨åˆ†
        for param in self.model.unet.parameters():
            param.requires_grad = False
        for param in self.model.diffusion.parameters():
            param.requires_grad = False
        
        # é‡æ–°åˆ›å»ºä¼˜åŒ–å™¨ï¼ŒåªåŒ…å«çœŸæ­£éœ€è¦è®­ç»ƒçš„å‚æ•°
        trainable_params = [p for p in self.model.parameters() if p.requires_grad]
        
        self.optimizer = torch.optim.AdamW(
            trainable_params,
            lr=self.optimizer.param_groups[0]['lr'],
            weight_decay=self.optimizer.param_groups[0]['weight_decay']
        )
        
        print(f"âœ… é‡æ–°åˆ›å»ºä¼˜åŒ–å™¨ï¼Œå¯è®­ç»ƒå‚æ•°: {sum(p.numel() for p in trainable_params):,}")

    def _ensure_frozen_modules_eval(self):
        """ç¡®ä¿å†»ç»“çš„æ¨¡å—å¤„äºevalæ¨¡å¼"""
        # ç¼–ç å™¨
        self.model.encoder.eval()
        for param in self.model.encoder.parameters():
            param.requires_grad = False
        
        # é€Ÿåº¦è§£ç å™¨å’ŒæŠ•å½±å™¨
        self.model.dual_decoder.velocity_decoder.eval()
        self.model.dual_decoder.velocity_projector.eval()
        
        for param in self.model.dual_decoder.velocity_decoder.parameters():
            param.requires_grad = False
        for param in self.model.dual_decoder.velocity_projector.parameters():
            param.requires_grad = False

    def compute_loss(self, pred_velocity: torch.Tensor, pred_seismic: torch.Tensor,
                    gt_velocity: torch.Tensor, gt_seismic: torch.Tensor) -> Tuple[torch.Tensor, Dict[str, float]]:
        """è®¡ç®—æŸå¤±"""
        # é€Ÿåº¦æŸå¤±ï¼ˆç”¨äºç›‘æ§ï¼Œä¸å‚ä¸è®­ç»ƒï¼‰
        velocity_l1 = self.l1_loss(pred_velocity, gt_velocity)
        velocity_l2 = self.l2_loss(pred_velocity, gt_velocity)
        velocity_loss = self.lambda_g1v * velocity_l1 + self.lambda_g2v * velocity_l2
        
        # åœ°éœ‡æŸå¤±ï¼ˆä¸»è¦è®­ç»ƒç›®æ ‡ï¼‰
        seismic_l1 = self.l1_loss(pred_seismic, gt_seismic)
        seismic_l2 = self.l2_loss(pred_seismic, gt_seismic)
        seismic_loss = self.lambda_g1v * seismic_l1 + self.lambda_g2v * seismic_l2
        
        # æ€»æŸå¤±ï¼ˆåªè®­ç»ƒåœ°éœ‡é‡æ„ï¼‰
        total_loss = seismic_loss
        
        loss_dict = {
            'velocity_loss': velocity_loss.item(),
            'seismic_loss': seismic_loss.item(),
            'velocity_l1': velocity_l1.item(),
            'velocity_l2': velocity_l2.item(),
            'seismic_l1': seismic_l1.item(),
            'seismic_l2': seismic_l2.item()
        }
        
        return total_loss, loss_dict

    def train_one_epoch(self, epoch: int, print_freq: int = 50) -> Dict[str, float]:
        """è®­ç»ƒä¸€ä¸ªepoch"""
        self.model.train()
        
        # å¼ºåˆ¶ç¡®ä¿å†»ç»“çš„æ¨¡å—ä¿æŒå†»ç»“çŠ¶æ€
        self._ensure_frozen_modules_eval()
                
        metric_logger = MetricLogger(delimiter="  ")
        metric_logger.add_meter('lr', SmoothedValue(window_size=1, fmt='{value}'))
        metric_logger.add_meter('samples/s', SmoothedValue(window_size=10, fmt='{value:.3f}'))
        header = f'Finetune Epoch: [{epoch + 1}]'
        
        # ç”¨äºè®¡ç®—SSIM
        velocity_tensors = []
        pred_velocity_tensors = []
        
        # æ¯ä¸ªepochå¼€å§‹æ—¶éªŒè¯å‚æ•°çŠ¶æ€ï¼ˆä»…ç¬¬ä¸€ä¸ªepochå’Œæ¯10ä¸ªepochï¼‰
        if epoch == 0 or (epoch + 1) % 10 == 0:
            print(f"\nEpoch {epoch + 1} å‚æ•°çŠ¶æ€æ£€æŸ¥:")
            self._quick_param_check()
        
        # ä½¿ç”¨é…å¯¹æ•°æ®è¿›è¡Œè®­ç»ƒ
        for batch_idx, (seismic, velocity) in enumerate(metric_logger.log_every(self.paired_loader, print_freq, header)):
            start_time = time.time()
            
            seismic = seismic.to(self.device, dtype=torch.float)
            velocity = velocity.to(self.device, dtype=torch.float)
            
            # å‰å‘ä¼ æ’­ - é‡æ„
            pred_velocity, pred_seismic = self.model.reconstruct(velocity)
            
            # è®¡ç®—æŸå¤±
            loss, loss_dict = self.compute_loss(pred_velocity, pred_seismic, velocity, seismic)
            
            # åå‘ä¼ æ’­å‰å†æ¬¡ç¡®ä¿å‚æ•°å†»ç»“
            if batch_idx == 0:  # åªåœ¨ç¬¬ä¸€ä¸ªbatchæ£€æŸ¥
                self._ensure_frozen_modules_eval()
            
            # åå‘ä¼ æ’­
            self.optimizer.zero_grad()
            loss.backward()
            
            # æ£€æŸ¥æ¢¯åº¦ï¼ˆå¯é€‰ï¼Œä»…è°ƒè¯•æ—¶ï¼‰
            if batch_idx == 0 and epoch == 0:
                self._check_gradients()
            
            self.optimizer.step()
            self.scheduler.step()
            
            # è®°å½•æŒ‡æ ‡
            batch_size = velocity.shape[0]
            metric_logger.update(
                loss=loss.item(),
                lr=self.optimizer.param_groups[0]['lr'],
                **loss_dict
            )
            metric_logger.meters['samples/s'].update(batch_size / (time.time() - start_time))
            
            # æ”¶é›†å¼ é‡ç”¨äºSSIMè®¡ç®—
            velocity_tensors.append(velocity.detach())
            pred_velocity_tensors.append(pred_velocity.detach())
            
            # è®°å½•åˆ°wandb
            if self.use_wandb:
                wandb.log({
                    "train/loss": loss.item(),
                    "train/learning_rate": self.optimizer.param_groups[0]['lr'],
                    "train/step": self.step,
                    **{f"train/{k}": v for k, v in loss_dict.items()}
                })
            
            self.step += 1
        
        # è®¡ç®—SSIM
        all_velocity = torch.cat(velocity_tensors, dim=0)
        all_pred_velocity = torch.cat(pred_velocity_tensors, dim=0)
        ssim_loss = SSIM(window_size=11)
        ssim_value = ssim_loss(all_velocity / 2 + 0.5, all_pred_velocity / 2 + 0.5)
        
        epoch_metrics = {
            'train_loss': metric_logger.meters['loss'].global_avg,
            'train_ssim': ssim_value.item(),
            'train_seismic_loss': metric_logger.meters['seismic_loss'].global_avg,
            'learning_rate': self.optimizer.param_groups[0]['lr']
        }
        
        print(f'å¾®è°ƒè®­ç»ƒ SSIM: {ssim_value.item():.4f}')
        print(f'å¾®è°ƒè®­ç»ƒ åœ°éœ‡æŸå¤±: {metric_logger.meters["seismic_loss"].global_avg:.4f}')
        print(f'å¾®è°ƒè®­ç»ƒ é€Ÿåº¦æŸå¤±: {metric_logger.meters["velocity_loss"].global_avg:.4f}')
        
        return epoch_metrics

    def _quick_param_check(self):
        """å¿«é€Ÿå‚æ•°çŠ¶æ€æ£€æŸ¥"""
        velocity_trainable = sum(p.numel() for name, p in self.model.named_parameters() 
                               if p.requires_grad and 'velocity' in name and ('decoder' in name or 'projector' in name))
        encoder_trainable = sum(p.numel() for name, p in self.model.named_parameters() 
                              if p.requires_grad and 'encoder' in name)
        seismic_trainable = sum(p.numel() for name, p in self.model.named_parameters() 
                              if p.requires_grad and 'seismic' in name and ('decoder' in name or 'projector' in name))
        
        if velocity_trainable > 0 or encoder_trainable > 0:
            print(f"âš ï¸  å‚æ•°æ³„æ¼æ£€æµ‹: ç¼–ç å™¨={encoder_trainable}, é€Ÿåº¦è§£ç å™¨={velocity_trainable}")
            self._force_freeze_parameters()
        else:
            print(f"âœ… å‚æ•°çŠ¶æ€æ­£å¸¸: åœ°éœ‡è§£ç å™¨={seismic_trainable}")

    def _check_gradients(self):
        """æ£€æŸ¥æ¢¯åº¦çŠ¶æ€ï¼ˆè°ƒè¯•ç”¨ï¼‰"""
        print("\né¦–æ¬¡å‰å‘ä¼ æ’­æ¢¯åº¦æ£€æŸ¥:")
        velocity_grads = []
        seismic_grads = []
        
        for name, param in self.model.named_parameters():
            if param.grad is not None and param.grad.norm() > 1e-8:
                if 'velocity' in name and ('decoder' in name or 'projector' in name):
                    velocity_grads.append(name)
                elif 'seismic' in name and ('decoder' in name or 'projector' in name):
                    seismic_grads.append(name)
        
        if velocity_grads:
            print(f"âŒ é€Ÿåº¦è§£ç å™¨æœ‰æ¢¯åº¦çš„å‚æ•°: {len(velocity_grads)}")
            for name in velocity_grads[:3]:  # åªæ˜¾ç¤ºå‰3ä¸ª
                print(f"  {name}")
        else:
            print("âœ… é€Ÿåº¦è§£ç å™¨æ— æ¢¯åº¦")
        
        print(f"âœ… åœ°éœ‡è§£ç å™¨æœ‰æ¢¯åº¦çš„å‚æ•°: {len(seismic_grads)}")

    def evaluate(self, epoch: int) -> Dict[str, float]:
        """è¯„ä¼°æ¨¡å‹"""
        self.model.eval()
        
        total_loss = 0.0
        total_seismic_loss = 0.0
        velocity_tensors = []
        pred_velocity_tensors = []
        
        with torch.no_grad():
            for seismic, velocity in self.test_loader:
                seismic = seismic.to(self.device, dtype=torch.float)
                velocity = velocity.to(self.device, dtype=torch.float)
                
                # é‡æ„
                pred_velocity, pred_seismic = self.model.reconstruct(velocity)
                
                # è®¡ç®—æŸå¤±
                loss, loss_dict = self.compute_loss(pred_velocity, pred_seismic, velocity, seismic)
                total_loss += loss.item()
                total_seismic_loss += loss_dict['seismic_loss']
                
                # æ”¶é›†å¼ é‡
                velocity_tensors.append(velocity)
                pred_velocity_tensors.append(pred_velocity)
        
        # è®¡ç®—å¹³å‡æŸå¤±
        avg_loss = total_loss / len(self.test_loader)
        avg_seismic_loss = total_seismic_loss / len(self.test_loader)
        
        # è®¡ç®—SSIM
        all_velocity = torch.cat(velocity_tensors, dim=0)
        all_pred_velocity = torch.cat(pred_velocity_tensors, dim=0)
        ssim_loss = SSIM(window_size=11)
        ssim_value = ssim_loss(all_velocity / 2 + 0.5, all_pred_velocity / 2 + 0.5)
        
        eval_metrics = {
            'val_loss': avg_loss,
            'val_seismic_loss': avg_seismic_loss,
            'val_ssim': ssim_value.item()
        }
        
        print(f'éªŒè¯æŸå¤±: {avg_loss:.4f}, åœ°éœ‡æŸå¤±: {avg_seismic_loss:.4f}, éªŒè¯SSIM: {ssim_value.item():.4f}')
        
        if self.use_wandb:
            wandb.log({f"val/{k}": v for k, v in eval_metrics.items()})
        
        return eval_metrics

    def save_checkpoint(self, epoch: int, is_best: bool = False) -> None:
        """ä¿å­˜æ£€æŸ¥ç‚¹"""
        checkpoint = {
            'epoch': epoch,
            'step': self.step,
            'model': self.model.state_dict(),
            'optimizer': self.optimizer.state_dict(),
            'scheduler': self.scheduler.state_dict(),
            'best_ssim': self.best_ssim,
            'best_loss': self.best_loss
        }
        
        filepath = os.path.join(self.output_path, f'finetune_checkpoint_epoch_{epoch + 1}.pth')
        save_checkpoint(checkpoint, filepath, is_best)

    def train(self, epochs: int, val_every: int = 10, print_freq: int = 50) -> None:
        """è®­ç»ƒä¸»å¾ªç¯"""
        print("å¼€å§‹åœ°éœ‡è§£ç å™¨å¾®è°ƒè®­ç»ƒ...")
        start_time = time.time()
        
        for epoch in range(epochs):
            print(f"\n{'='*50}")
            print(f"Finetune Epoch: {epoch + 1}/{epochs}")
            
            # è®­ç»ƒ
            train_metrics = self.train_one_epoch(epoch, print_freq)
            
            # éªŒè¯
            if (epoch + 1) % val_every == 0:
                val_metrics = self.evaluate(epoch)
                
                # æ£€æŸ¥æ˜¯å¦æ˜¯æœ€ä½³æ¨¡å‹ï¼ˆä¸»è¦çœ‹åœ°éœ‡æŸå¤±ï¼‰
                is_best = val_metrics['val_seismic_loss'] < self.best_loss
                if is_best:
                    self.best_loss = val_metrics['val_seismic_loss']
                    self.best_ssim = val_metrics['val_ssim']
                    print(f"æ–°çš„æœ€ä½³æ¨¡å‹! åœ°éœ‡æŸå¤±: {self.best_loss:.4f}")
                
                # ä¿å­˜æ£€æŸ¥ç‚¹
                self.save_checkpoint(epoch, is_best)
                
                print(f"å½“å‰æœ€ä½³åœ°éœ‡æŸå¤±: {self.best_loss:.4f}")
        
        # è®­ç»ƒå®Œæˆ
        total_time = time.time() - start_time
        total_time_str = str(datetime.timedelta(seconds=int(total_time)))
        print(f'\nå¾®è°ƒè®­ç»ƒå®Œæˆ! æ€»æ—¶é—´: {total_time_str}')
        
        if self.use_wandb:
            wandb.finish()


def create_trainer_from_args(args) -> FinetuneTrainer:
    """ä»å‘½ä»¤è¡Œå‚æ•°åˆ›å»ºå¾®è°ƒè®­ç»ƒå™¨"""
    return FinetuneTrainer(
        seismic_folder=args.train_data,
        velocity_folder=args.train_label,
        dataset_name=args.dataset,
        output_path=args.output_path,
        checkpoint_path=args.checkpoint_path,
        num_data=args.num_data,
        paired_num=args.paired_num,
        batch_size=args.batch_size,
        learning_rate=args.lr,
        weight_decay=args.weight_decay,
        lr_gamma=args.lr_gamma,
        lr_milestones=args.lr_milestones,
        warmup_epochs=args.lr_warmup_epochs,
        num_workers=args.workers,
        encoder_dim=args.encoder_dim,
        lambda_g1v=args.lambda_g1v,
        lambda_g2v=args.lambda_g2v,
        use_wandb=args.use_wandb,
        wandb_project=args.proj_name,
        preload=args.preload,
        preload_workers=args.preload_workers,
        cache_size=args.cache_size,
        use_memmap=args.use_memmap
    ) 