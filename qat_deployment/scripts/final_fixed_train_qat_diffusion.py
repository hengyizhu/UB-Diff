#!/usr/bin/env python3
"""
æœ€ç»ˆä¿®å¤ç‰ˆæœ¬ - QATæ‰©æ•£æ¨¡å‹è®­ç»ƒè„šæœ¬

æ­£ç¡®å¤„ç†QATè§£ç å™¨çŠ¶æ€åŠ è½½å’Œä¿å­˜
"""

import os
import sys
import argparse
import torch
import torch.quantization as quant

# æ·»åŠ é¡¹ç›®è·¯å¾„åˆ°sys.path
current_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.dirname(os.path.dirname(current_dir))
if project_root not in sys.path:
    sys.path.insert(0, project_root)

from model.ub_diff import UBDiff
from model.data import create_dataloaders
from qat_deployment.models import QuantizedUBDiff, prepare_qat_model, convert_to_quantized
from qat_deployment.trainers import QATDiffusionTrainer


def parse_args():
    parser = argparse.ArgumentParser(description='QATæ‰©æ•£æ¨¡å‹è®­ç»ƒï¼ˆæœ€ç»ˆä¿®å¤ç‰ˆæœ¬ï¼‰')
    
    # æ•°æ®å‚æ•°
    parser.add_argument('--train_data', type=str, required=True,
                        help='è®­ç»ƒåœ°éœ‡æ•°æ®è·¯å¾„')
    parser.add_argument('--train_label', type=str, required=True,
                        help='è®­ç»ƒé€Ÿåº¦åœºæ•°æ®è·¯å¾„')
    parser.add_argument('--val_data', type=str, default=None,
                        help='éªŒè¯åœ°éœ‡æ•°æ®è·¯å¾„')
    parser.add_argument('--val_label', type=str, default=None,
                        help='éªŒè¯é€Ÿåº¦åœºæ•°æ®è·¯å¾„')
    parser.add_argument('--dataset', type=str, default='curvefault-a',
                        help='æ•°æ®é›†åç§°')
    
    # æ¨¡å‹å‚æ•°
    parser.add_argument('--pretrained_path', type=str, required=True,
                        help='é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹è·¯å¾„')
    parser.add_argument('--decoder_checkpoint', type=str, required=True,
                        help='QATè§£ç å™¨æ£€æŸ¥ç‚¹è·¯å¾„')
    parser.add_argument('--encoder_dim', type=int, default=512,
                        help='ç¼–ç å™¨ç»´åº¦')
    parser.add_argument('--time_steps', type=int, default=256,
                        help='æ‰©æ•£æ—¶é—´æ­¥æ•°')
    
    # è®­ç»ƒå‚æ•°
    parser.add_argument('--batch_size', type=int, default=16,
                        help='æ‰¹æ¬¡å¤§å°')
    parser.add_argument('--epochs', type=int, default=100,
                        help='è®­ç»ƒè½®æ•°')
    parser.add_argument('--lr', type=float, default=8e-5,
                        help='å­¦ä¹ ç‡')
    parser.add_argument('--weight_decay', type=float, default=1e-4,
                        help='æƒé‡è¡°å‡')
    parser.add_argument('--gradient_clip', type=float, default=1.0,
                        help='æ¢¯åº¦è£å‰ª')
    parser.add_argument('--num_workers', type=int, default=16,
                        help='æ•°æ®åŠ è½½çº¿ç¨‹æ•°')
    
    # é‡åŒ–å‚æ•°
    parser.add_argument('--quantize_diffusion', action='store_true',
                        help='æ˜¯å¦é‡åŒ–æ‰©æ•£æ¨¡å‹')
    parser.add_argument('--backend', type=str, default='qnnpack',
                        choices=['qnnpack', 'fbgemm'],
                        help='é‡åŒ–åç«¯')
    parser.add_argument('--convert_conv1d', action='store_true',
                        help='æ˜¯å¦è½¬æ¢1Då·ç§¯ä¸º2Då·ç§¯ä»¥è·å¾—æ›´å¥½çš„é‡åŒ–æ”¯æŒ')
    parser.add_argument('--use_aggressive_quantization', action='store_true',
                        help='æ˜¯å¦ä½¿ç”¨æ›´æ¿€è¿›çš„é‡åŒ–é…ç½®')
    
    # å…¶ä»–å‚æ•°
    parser.add_argument('--checkpoint_dir', type=str, default='./checkpoints/qat_diffusion',
                        help='æ£€æŸ¥ç‚¹ä¿å­˜ç›®å½•')
    parser.add_argument('--save_every', type=int, default=10,
                        help='ä¿å­˜æ£€æŸ¥ç‚¹é¢‘ç‡')
    parser.add_argument('--validate_every', type=int, default=5,
                        help='éªŒè¯é¢‘ç‡ï¼ˆæ¯å¤šå°‘ä¸ªepochéªŒè¯ä¸€æ¬¡ï¼‰')
    parser.add_argument('--device', type=str, default='cuda',
                        help='è®­ç»ƒè®¾å¤‡')
    parser.add_argument('--use_wandb', action='store_true',
                        help='æ˜¯å¦ä½¿ç”¨WandBè®°å½•')
    parser.add_argument('--wandb_project', type=str, default='ub-diff-qat',
                        help='WandBé¡¹ç›®åç§°')
    
    return parser.parse_args()


def load_encoder(pretrained_path: str, device: str) -> torch.nn.Module:
    """åŠ è½½é¢„è®­ç»ƒçš„ç¼–ç å™¨"""
    model = UBDiff(
        in_channels=1,
        encoder_dim=512,
        velocity_channels=1,
        seismic_channels=5,
        dim_mults=(1, 2, 4, 8)
    )
    
    checkpoint = torch.load(pretrained_path, map_location='cpu', weights_only=False)
    if 'model' in checkpoint:
        state_dict = checkpoint['model']
    else:
        state_dict = checkpoint
    
    model.load_state_dict(state_dict, strict=False)
    encoder = model.encoder
    encoder.eval()
    
    return encoder.to(device)


def create_simple_dataloader(seismic_path: str, velocity_path: str, 
                            dataset_name: str, batch_size: int = 32,
                            shuffle: bool = True, num_workers: int = 4):
    """åˆ›å»ºç®€åŒ–çš„æ•°æ®åŠ è½½å™¨"""
    from model.data import DatasetConfig, SeismicVelocityDataset
    from torch.utils.data import DataLoader
    
    # è·å–æ•°æ®é›†é…ç½®
    config = DatasetConfig()
    ctx = config.get_dataset_info(dataset_name)
    seismic_transform, velocity_transform = config.get_transforms(dataset_name, k=1.0)
    
    # åˆ¤æ–­æ˜¯å¦ä¸ºæ–­å±‚æ—æ•°æ®é›†
    fault_family = dataset_name in ['flatfault-a', 'curvefault-a', 'flatfault-b', 'curvefault-b']
    
    # åˆ›å»ºæ•°æ®é›†
    dataset = SeismicVelocityDataset(
        seismic_folder=seismic_path,
        velocity_folder=velocity_path,
        seismic_transform=seismic_transform,
        velocity_transform=velocity_transform,
        fault_family=fault_family,
        preload=True
    )
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    dataloader = DataLoader(
        dataset,
        batch_size=batch_size,
        shuffle=shuffle,
        num_workers=num_workers,
        pin_memory=True,
        drop_last=False
    )
    
    return dataloader


def count_qat_modules(model, module_path=""):
    """ç»Ÿè®¡æ¨¡å‹ä¸­QATæ¨¡å—çš„æ•°é‡"""
    qat_count = 0
    total_modules = 0
    
    for name, module in model.named_modules():
        total_modules += 1
        if hasattr(module, 'qconfig') and module.qconfig is not None:
            qat_count += 1
    
    return qat_count, total_modules


def load_qat_decoder_correctly(model, decoder_checkpoint_path):
    """æ­£ç¡®åŠ è½½QATè§£ç å™¨çŠ¶æ€"""
    print("\n=== æ­£ç¡®åŠ è½½QATè§£ç å™¨ ===")
    print(f"è§£ç å™¨è·¯å¾„: {decoder_checkpoint_path}")
    
    # åŠ è½½QATè§£ç å™¨æ£€æŸ¥ç‚¹
    decoder_checkpoint = torch.load(decoder_checkpoint_path, map_location='cpu')
    decoder_state = decoder_checkpoint['model_state_dict']
    
    print(f"ğŸ“Š QATè§£ç å™¨æ–‡ä»¶åŒ…å« {len(decoder_state)} ä¸ªå‚æ•°")
    
    # åˆ†æQATå‚æ•°
    qat_params = [k for k in decoder_state.keys() 
                  if 'fake_quant' in k or 'observer' in k or 'activation_post_process' in k]
    normal_params = [k for k in decoder_state.keys() 
                     if not ('fake_quant' in k or 'observer' in k or 'activation_post_process' in k)]
    
    print(f"âœ… å…¶ä¸­ {len(qat_params)} ä¸ªæ˜¯QATå‚æ•°")
    print(f"ğŸ“¦ å…¶ä¸­ {len(normal_params)} ä¸ªæ˜¯æ™®é€šå‚æ•°")
    
    # æ£€æŸ¥åŠ è½½å‰çš„QATçŠ¶æ€
    decoder_qat_before, decoder_total = count_qat_modules(model.decoder)
    print(f"åŠ è½½å‰è§£ç å™¨QATçŠ¶æ€:")
    print(f"  ğŸ“¦ decoder: {decoder_qat_before}/{decoder_total} ä¸ªæ¨¡å—æœ‰QATé…ç½®")
    
    # æ–¹æ³•1: å°è¯•ç›´æ¥åŠ è½½æ‰€æœ‰çŠ¶æ€ï¼ˆåŒ…æ‹¬QATçŠ¶æ€ï¼‰
    print("\nğŸ”„ æ–¹æ³•1: ç›´æ¥åŠ è½½æ‰€æœ‰QATçŠ¶æ€...")
    result = model.decoder.load_state_dict(decoder_state, strict=False)
    print(f"ğŸ“‹ åŠ è½½ç»“æœ:")
    print(f"  ç¼ºå¤±çš„keys: {len(result.missing_keys)}")
    print(f"  æ„å¤–çš„keys: {len(result.unexpected_keys)}")
    
    # æ£€æŸ¥QATçŠ¶æ€æ˜¯å¦æ­£ç¡®åŠ è½½
    decoder_qat_after, _ = count_qat_modules(model.decoder)
    print(f"åŠ è½½åè§£ç å™¨QATçŠ¶æ€:")
    print(f"  ğŸ“¦ decoder: {decoder_qat_after}/{decoder_total} ä¸ªæ¨¡å—æœ‰QATé…ç½®")
    
    if decoder_qat_after == 0:
        print("âŒ æ–¹æ³•1å¤±è´¥ï¼Œå°è¯•æ–¹æ³•2...")
        
        # æ–¹æ³•2: å…ˆåº”ç”¨é‡åŒ–é…ç½®ï¼Œå†åŠ è½½æƒé‡
        print("\nğŸ”„ æ–¹æ³•2: å…ˆé…ç½®QATï¼Œå†åŠ è½½æƒé‡...")
        
        # ä¸ºè§£ç å™¨åº”ç”¨QATé…ç½®
        model.decoder.qconfig = torch.quantization.get_default_qat_qconfig('qnnpack')
        torch.quantization.prepare_qat(model.decoder, inplace=True)
        
        # å†æ¬¡æ£€æŸ¥QATçŠ¶æ€
        decoder_qat_prepared, _ = count_qat_modules(model.decoder)
        print(f"QATé…ç½®åè§£ç å™¨çŠ¶æ€:")
        print(f"  ğŸ“¦ decoder: {decoder_qat_prepared}/{decoder_total} ä¸ªæ¨¡å—æœ‰QATé…ç½®")
        
        # å†æ¬¡å°è¯•åŠ è½½æƒé‡
        result = model.decoder.load_state_dict(decoder_state, strict=False)
        print(f"ğŸ“‹ ç¬¬äºŒæ¬¡åŠ è½½ç»“æœ:")
        print(f"  ç¼ºå¤±çš„keys: {len(result.missing_keys)}")
        print(f"  æ„å¤–çš„keys: {len(result.unexpected_keys)}")
        
        # æœ€ç»ˆæ£€æŸ¥
        decoder_qat_final, _ = count_qat_modules(model.decoder)
        print(f"æœ€ç»ˆè§£ç å™¨QATçŠ¶æ€:")
        print(f"  ğŸ“¦ decoder: {decoder_qat_final}/{decoder_total} ä¸ªæ¨¡å—æœ‰QATé…ç½®")
        
        if decoder_qat_final > 0:
            print("âœ… æ–¹æ³•2æˆåŠŸï¼QATè§£ç å™¨çŠ¶æ€å·²æ­£ç¡®åŠ è½½")
        else:
            print("âŒ æ–¹æ³•2ä¹Ÿå¤±è´¥äº†")
    else:
        print("âœ… æ–¹æ³•1æˆåŠŸï¼QATè§£ç å™¨çŠ¶æ€å·²æ­£ç¡®åŠ è½½")
    
    return decoder_qat_after > 0 or (decoder_qat_after == 0 and 'decoder_qat_final' in locals() and decoder_qat_final > 0)


def main():
    args = parse_args()
    
    # è®¾ç½®è®¾å¤‡
    device = torch.device(args.device)
    
    # åˆå§‹åŒ–WandB
    if args.use_wandb:
        import wandb
        wandb.init(
            project=args.wandb_project,
            name='qat-diffusion-final-fix',
            config=vars(args)
        )
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    print("åˆ›å»ºæ•°æ®åŠ è½½å™¨...")
    train_loader = create_simple_dataloader(args.train_data, args.train_label, args.dataset, args.batch_size, True, args.num_workers)
    
    val_loader = None
    if args.val_data and args.val_label:
        val_loader = create_simple_dataloader(args.val_data, args.val_label, args.dataset, args.batch_size, False, args.num_workers)
    
    # åŠ è½½ç¼–ç å™¨ï¼ˆç”¨äºç”ŸæˆçœŸå®æ½œåœ¨è¡¨ç¤ºï¼‰
    print("åŠ è½½ç¼–ç å™¨...")
    encoder = load_encoder(args.pretrained_path, device)
    
    # åˆ›å»ºé‡åŒ–æ¨¡å‹ï¼ˆä¸å¯ç”¨è§£ç å™¨é‡åŒ–ï¼Œç¨åæ‰‹åŠ¨åŠ è½½ï¼‰
    print("\n=== åˆ›å»ºQuantizedUBDiffæ¨¡å‹ ===")
    model = QuantizedUBDiff(
        encoder_dim=args.encoder_dim,
        velocity_channels=1,
        seismic_channels=5,
        dim_mults=(1, 2, 4, 8),
        time_steps=args.time_steps,
        quantize_diffusion=args.quantize_diffusion,
        quantize_decoder=False  # å…ˆä¸å¯ç”¨ï¼Œæ‰‹åŠ¨åŠ è½½
    )
    
    # åŠ è½½é¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹æƒé‡
    print("\n=== åŠ è½½æ‰©æ•£æ¨¡å‹æƒé‡ ===")
    checkpoint = torch.load(args.pretrained_path, map_location='cpu', weights_only=False)
    if 'model' in checkpoint:
        state_dict = checkpoint['model']
    else:
        state_dict = checkpoint
    
    # åŠ è½½æ‰©æ•£ç›¸å…³æƒé‡
    diffusion_state = {}
    for key, value in state_dict.items():
        if 'diffusion' in key or 'unet' in key:
            diffusion_state[key] = value
    
    model.load_state_dict(diffusion_state, strict=False)
    print("âœ… æ‰©æ•£æ¨¡å‹æƒé‡åŠ è½½å®Œæˆ")
    
    # æ­£ç¡®åŠ è½½QATè§£ç å™¨
    qat_success = load_qat_decoder_correctly(model, args.decoder_checkpoint)
    
    if not qat_success:
        print("âš ï¸ è­¦å‘Šï¼šQATè§£ç å™¨åŠ è½½å¯èƒ½ä¸å®Œæ•´ï¼Œä½†ç»§ç»­è®­ç»ƒ...")
    
    # å¯¹æ‰©æ•£æ¨¡å‹åº”ç”¨é‡åŒ–ç­–ç•¥
    if args.quantize_diffusion:
        print(f"\n=== åº”ç”¨æ‰©æ•£æ¨¡å‹é‡åŒ–ç­–ç•¥ ===")
        print(f"é‡åŒ–åç«¯: {args.backend}")
        print(f"è½¬æ¢1Då·ç§¯: {args.convert_conv1d}")
        print(f"æ¿€è¿›é‡åŒ–: {args.use_aggressive_quantization}")
        
        quantization_report = model.apply_improved_quantization(
            backend=args.backend,
            convert_conv1d=args.convert_conv1d,
            use_aggressive_config=args.use_aggressive_quantization
        )
        
        # ä¿å­˜é‡åŒ–æŠ¥å‘Š
        os.makedirs(args.checkpoint_dir, exist_ok=True)
        report_path = os.path.join(args.checkpoint_dir, 'quantization_analysis_final.txt')
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write("æœ€ç»ˆä¿®å¤ç‰ˆæœ¬é‡åŒ–åˆ†ææŠ¥å‘Š\n")
            f.write("=" * 50 + "\n\n")
            
            f.write("é‡åŒ–åˆ†æç»“æœ:\n")
            f.write(f"  æ€»æ¨¡å—æ•°: {quantization_report['total_modules']}\n")
            f.write(f"  æ€»å‚æ•°æ•°: {quantization_report['total_params']:,}\n")
            f.write(f"  å¯é‡åŒ–æ¨¡å—: {quantization_report['quantizable_modules']}\n")
            f.write(f"  å¯é‡åŒ–å‚æ•°: {quantization_report['quantizable_params']:,}\n")
            f.write(f"  å¯é‡åŒ–æ¨¡å—æ¯”ä¾‹: {quantization_report['quantizable_ratio']:.1%}\n")
            f.write(f"  å¯é‡åŒ–å‚æ•°æ¯”ä¾‹: {quantization_report['quantizable_param_ratio']:.1%}\n")
            f.write(f"  1Då·ç§¯æ•°é‡: {quantization_report['conv1d_count']}\n")
            
            if quantization_report['converted_conv1d'] > 0:
                f.write(f"\nâœ“ æˆåŠŸè½¬æ¢ {quantization_report['converted_conv1d']} ä¸ª1Då·ç§¯å±‚\n")
        
        print(f"é‡åŒ–åˆ†ææŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_path}")
    else:
        print("è·³è¿‡æ‰©æ•£æ¨¡å‹é‡åŒ–")
    
    # æœ€ç»ˆQATçŠ¶æ€æ£€æŸ¥
    print(f"\n=== æœ€ç»ˆæ¨¡å‹QATçŠ¶æ€æ£€æŸ¥ ===")
    decoder_qat, decoder_total = count_qat_modules(model.decoder)
    diffusion_qat, diffusion_total = count_qat_modules(model.diffusion)
    unet_qat, unet_total = count_qat_modules(model.unet)
    
    print(f"  ğŸ“¦ decoder: {decoder_qat}/{decoder_total} ä¸ªæ¨¡å—æœ‰QATé…ç½®")
    print(f"  ğŸ“¦ diffusion: {diffusion_qat}/{diffusion_total} ä¸ªæ¨¡å—æœ‰QATé…ç½®") 
    print(f"  ğŸ“¦ unet: {unet_qat}/{unet_total} ä¸ªæ¨¡å—æœ‰QATé…ç½®")
    
    # ç§»åŠ¨åˆ°è®¾å¤‡
    model = model.to(device)
    
    # åˆ›å»ºè®­ç»ƒå™¨
    trainer = QATDiffusionTrainer(
        model=model,
        encoder_model=encoder,
        device=device,
        learning_rate=args.lr,
        weight_decay=args.weight_decay,
        gradient_clip=args.gradient_clip,
        use_wandb=args.use_wandb
    )
    
    # è®­ç»ƒæ‰©æ•£æ¨¡å‹
    print("\n=== è®­ç»ƒé‡åŒ–æ‰©æ•£æ¨¡å‹ ===")
    trainer.train(
        train_loader=train_loader,
        val_loader=val_loader,
        epochs=args.epochs,
        checkpoint_dir=args.checkpoint_dir,
        save_every=args.save_every,
        validate_every=args.validate_every
    )
    
    # ä¿å­˜æœ€ä½³ç»„åˆæ¨¡å‹
    print("\nä¿å­˜æœ€ä½³QATæ‰©æ•£æ¨¡å‹...")
    
    best_diffusion_path = os.path.join(args.checkpoint_dir, 'best_diffusion.pt')
    
    # å¦‚æœå­˜åœ¨æœ€ä½³æ¨¡å‹ï¼ŒåŠ è½½å®ƒ
    if os.path.exists(best_diffusion_path):
        print(f"åŠ è½½æœ€ä½³æ‰©æ•£æ¨¡å‹: {best_diffusion_path}")
        best_checkpoint = torch.load(best_diffusion_path, map_location='cpu')
        best_model_state = best_checkpoint['model_state_dict']
    else:
        print("è­¦å‘Šï¼šæœªæ‰¾åˆ°æœ€ä½³æ‰©æ•£æ¨¡å‹ï¼Œä½¿ç”¨å½“å‰æ¨¡å‹çŠ¶æ€")
        best_model_state = model.state_dict()
    
    # ä¿å­˜æœ€ä½³æ¨¡å‹
    final_checkpoint = {
        'model_state_dict': best_model_state,
        'args': vars(args),
        'qat_status': {
            'decoder_qat_modules': decoder_qat,
            'diffusion_qat_modules': diffusion_qat,
            'unet_qat_modules': unet_qat,
        },
        'note': 'Best QAT diffusion model with properly loaded QAT decoder'
    }
    torch.save(
        final_checkpoint,
        os.path.join(args.checkpoint_dir, 'best_qat_diffusion_final.pt')
    )
    
    print(f"æœ€ç»ˆQATæ‰©æ•£æ¨¡å‹å·²ä¿å­˜åˆ°: {os.path.join(args.checkpoint_dir, 'best_qat_diffusion_final.pt')}")
    print("è®­ç»ƒå®Œæˆï¼")


if __name__ == '__main__':
    main() 