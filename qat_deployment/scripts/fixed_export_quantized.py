#!/usr/bin/env python3
"""
ä¿®å¤åçš„é‡åŒ–æ¨¡å‹å¯¼å‡ºè„šæœ¬

æ­£ç¡®å¯¼å‡ºçœŸæ­£çš„INT8é‡åŒ–æ¨¡å‹ï¼Œæ”¯æŒæ ‘è“æ´¾éƒ¨ç½²
"""

import os
import sys
import argparse
import torch
import torch.quantization as quant
from typing import Tuple

# æ·»åŠ é¡¹ç›®è·¯å¾„åˆ°sys.path
current_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.dirname(current_dir)
if project_root not in sys.path:
    sys.path.insert(0, project_root)

from qat_deployment.models import QuantizedUBDiff


def parse_args():
    parser = argparse.ArgumentParser(description='ä¿®å¤çš„é‡åŒ–æ¨¡å‹å¯¼å‡º')
    
    parser.add_argument('--checkpoint_path', type=str, required=True,
                        help='å®Œæ•´QATæ¨¡å‹æ£€æŸ¥ç‚¹è·¯å¾„')
    parser.add_argument('--output_dir', type=str, default='./exported_models_int8',
                        help='è¾“å‡ºç›®å½•')
    parser.add_argument('--model_name', type=str, default='ub_diff_int8',
                        help='å¯¼å‡ºæ¨¡å‹åç§°')
    parser.add_argument('--batch_size', type=int, default=1,
                        help='å¯¼å‡ºæ—¶çš„æ‰¹æ¬¡å¤§å°')
    parser.add_argument('--backend', type=str, default='qnnpack',
                        choices=['qnnpack', 'fbgemm'],
                        help='é‡åŒ–åç«¯')
    parser.add_argument('--test_generation', action='store_true',
                        help='æ˜¯å¦æµ‹è¯•ç”ŸæˆåŠŸèƒ½')
    parser.add_argument('--force_cpu', action='store_true',
                        help='å¼ºåˆ¶åœ¨CPUä¸Šè¿è¡Œï¼ˆæ¨èç”¨äºé‡åŒ–æ¨¡å‹ï¼‰')
    
    return parser.parse_args()


class SimpleQuantizedModel(torch.nn.Module):
    """ç”¨äºéƒ¨ç½²çš„ç®€åŒ–é‡åŒ–æ¨¡å‹
    
    ä¸“é—¨ä¸ºTorchScriptå…¼å®¹æ€§è®¾è®¡
    """
    
    def __init__(self, quantized_diffusion, quantized_decoder):
        super().__init__()
        self.diffusion = quantized_diffusion
        self.decoder = quantized_decoder
        
        # è®°å½•è¾“å‡ºç»´åº¦
        self.encoder_dim = 512
        
    def forward(self, noise_steps: int = 50) -> Tuple[torch.Tensor, torch.Tensor]:
        """ç®€åŒ–çš„å‰å‘ä¼ æ’­ï¼Œé€‚åˆTorchScript"""
        
        batch_size = 1  # å›ºå®šæ‰¹æ¬¡å¤§å°ä»¥ç®€åŒ–TorchScriptè¿½è¸ª
        device = next(self.parameters()).device
        
        # ä½¿ç”¨ç®€åŒ–çš„å™ªå£°é‡‡æ ·ï¼ˆé¿å…å¤æ‚çš„æ‰©æ•£å¾ªç¯ï¼‰
        # åœ¨å®é™…éƒ¨ç½²æ—¶ï¼Œå¯ä»¥æ›¿æ¢ä¸ºå®Œæ•´çš„æ‰©æ•£è¿‡ç¨‹
        z = torch.randn(batch_size, self.encoder_dim, device=device)
        z = z.view(batch_size, -1, 1, 1)
        
        # è§£ç 
        velocity, seismic = self.decoder(z)
        
        return velocity, seismic


def check_model_quantization_status(model):
    """æ£€æŸ¥æ¨¡å‹çš„é‡åŒ–çŠ¶æ€"""
    print(f"\nğŸ” æ£€æŸ¥æ¨¡å‹é‡åŒ–çŠ¶æ€")
    
    total_params = 0
    quantized_params = 0
    qat_params = 0
    
    state_dict = model.state_dict()
    
    for name, param in state_dict.items():
        total_params += 1
        dtype = str(param.dtype)
        
        if 'qint' in dtype or 'quint' in dtype:
            quantized_params += 1
            print(f"  âœ… é‡åŒ–å‚æ•°: {name} ({dtype})")
        elif 'fake_quant' in name or 'observer' in name or 'activation_post_process' in name:
            qat_params += 1
    
    print(f"\nğŸ“Š é‡åŒ–çŠ¶æ€ç»Ÿè®¡:")
    print(f"  æ€»å‚æ•°: {total_params}")
    print(f"  QATå‚æ•°: {qat_params}")  
    print(f"  çœŸæ­£é‡åŒ–å‚æ•°: {quantized_params}")
    
    if quantized_params > 0:
        print(f"  âœ… æ¨¡å‹åŒ…å«çœŸæ­£çš„é‡åŒ–å‚æ•°")
        return True
    elif qat_params > 0:
        print(f"  âš ï¸ æ¨¡å‹ä»…åŒ…å«QATå‚æ•°ï¼Œæœªè½¬æ¢ä¸ºé‡åŒ–")
        return False
    else:
        print(f"  âŒ æ¨¡å‹æœªé‡åŒ–")
        return False


def convert_qat_to_quantized(model):
    """å°†QATæ¨¡å‹è½¬æ¢ä¸ºçœŸæ­£çš„é‡åŒ–æ¨¡å‹"""
    print(f"\n=== è½¬æ¢QATä¸ºé‡åŒ–æ¨¡å‹ ===")
    
    model.eval()
    
    # åˆ†åˆ«è½¬æ¢å„ä¸ªç»„ä»¶
    converted_components = {}
    
    for component_name in ['decoder', 'diffusion', 'unet']:
        if hasattr(model, component_name):
            component = getattr(model, component_name)
            print(f"\nğŸ”„ è½¬æ¢ {component_name}...")
            
            # æ£€æŸ¥æ˜¯å¦æœ‰qconfig
            qconfig_count = 0
            for name, module in component.named_modules():
                if hasattr(module, 'qconfig') and module.qconfig is not None:
                    qconfig_count += 1
            
            print(f"  qconfigæ¨¡å—æ•°: {qconfig_count}")
            
            if qconfig_count > 0:
                try:
                    # è½¬æ¢ä¸ºé‡åŒ–æ¨¡å‹
                    converted_component = quant.convert(component, inplace=False)
                    
                    # æ£€æŸ¥è½¬æ¢ç»“æœ
                    converted_state = converted_component.state_dict()
                    quantized_params = sum(1 for param in converted_state.values() 
                                         if 'qint' in str(param.dtype) or 'quint' in str(param.dtype))
                    
                    if quantized_params > 0:
                        print(f"  âœ… {component_name} è½¬æ¢æˆåŠŸ: {quantized_params} ä¸ªé‡åŒ–å‚æ•°")
                        converted_components[component_name] = converted_component
                        setattr(model, component_name, converted_component)
                    else:
                        print(f"  âŒ {component_name} è½¬æ¢å¤±è´¥: æ— é‡åŒ–å‚æ•°")
                
                except Exception as e:
                    print(f"  âŒ {component_name} è½¬æ¢å‡ºé”™: {e}")
            else:
                print(f"  âš ï¸ {component_name} è·³è¿‡: æ— qconfig")
    
    if converted_components:
        print(f"\nâœ… æˆåŠŸè½¬æ¢ {len(converted_components)} ä¸ªç»„ä»¶")
        return True
    else:
        print(f"\nâŒ æ²¡æœ‰ç»„ä»¶æˆåŠŸè½¬æ¢")
        return False


def main():
    args = parse_args()
    
    print("ğŸš€ ä¿®å¤çš„é‡åŒ–æ¨¡å‹å¯¼å‡º")
    print("=" * 60)
    
    os.makedirs(args.output_dir, exist_ok=True)
    
    # è®¾ç½®è®¾å¤‡
    device = torch.device('cpu' if args.force_cpu else 'cuda')
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")
    
    # è®¾ç½®é‡åŒ–åç«¯
    torch.backends.quantized.engine = args.backend
    print(f"é‡åŒ–åç«¯: {args.backend}")
    
    # åŠ è½½QATæ¨¡å‹
    print(f"\nğŸ“¥ åŠ è½½QATæ¨¡å‹: {args.checkpoint_path}")
    
    try:
        checkpoint = torch.load(args.checkpoint_path, map_location='cpu')
        
        if 'model_state_dict' in checkpoint:
            model_state = checkpoint['model_state_dict']
            print("âœ… ä½¿ç”¨ model_state_dict")
        else:
            model_state = checkpoint
            print("âœ… ä½¿ç”¨æ•´ä¸ªæ£€æŸ¥ç‚¹")
            
    except Exception as e:
        print(f"âŒ åŠ è½½æ£€æŸ¥ç‚¹å¤±è´¥: {e}")
        return
    
    # åˆ›å»ºæ¨¡å‹
    print(f"\nğŸ—ï¸ åˆ›å»ºé‡åŒ–æ¨¡å‹")
    model = QuantizedUBDiff(
        encoder_dim=512,
        velocity_channels=1,
        seismic_channels=5,
        dim_mults=(1, 2, 4, 8),
        time_steps=256,
        quantize_diffusion=True,
        quantize_decoder=True
    )
    
    # é‡æ–°åº”ç”¨é‡åŒ–é…ç½®ï¼ˆä¸è®­ç»ƒæ—¶ä¿æŒä¸€è‡´ï¼‰
    print(f"\nâš™ï¸ é‡æ–°åº”ç”¨é‡åŒ–é…ç½®")
    try:
        quantization_report = model.apply_improved_quantization(
            backend=args.backend,
            convert_conv1d=True,
            use_aggressive_config=True
        )
        print(f"âœ… é‡åŒ–é…ç½®åº”ç”¨æˆåŠŸ")
    except Exception as e:
        print(f"âŒ é‡åŒ–é…ç½®å¤±è´¥: {e}")
        return
    
    # åŠ è½½æƒé‡
    print(f"\nğŸ“¥ åŠ è½½è®­ç»ƒåçš„æƒé‡")
    try:
        model.load_state_dict(model_state, strict=False)
        print(f"âœ… æƒé‡åŠ è½½æˆåŠŸ")
    except Exception as e:
        print(f"âŒ æƒé‡åŠ è½½å¤±è´¥: {e}")
        return
    
    # ç§»åŠ¨åˆ°æŒ‡å®šè®¾å¤‡
    model = model.to(device)
    
    # æ£€æŸ¥QATçŠ¶æ€
    is_qat = check_model_quantization_status(model)
    
    if not is_qat:
        print(f"âŒ æ¨¡å‹æœªæ­£ç¡®é‡åŒ–ï¼Œç»ˆæ­¢å¯¼å‡º")
        return
    
    # è½¬æ¢ä¸ºçœŸæ­£çš„é‡åŒ–æ¨¡å‹
    conversion_success = convert_qat_to_quantized(model)
    
    if not conversion_success:
        print(f"âŒ é‡åŒ–è½¬æ¢å¤±è´¥ï¼Œç»ˆæ­¢å¯¼å‡º")
        return
    
    # æœ€ç»ˆæ£€æŸ¥
    final_quantized = check_model_quantization_status(model)
    
    if not final_quantized:
        print(f"âŒ æœ€ç»ˆæ¨¡å‹æœªé‡åŒ–ï¼Œç»ˆæ­¢å¯¼å‡º")
        return
    
    print(f"âœ… æ¨¡å‹å·²æˆåŠŸè½¬æ¢ä¸ºINT8é‡åŒ–ç‰ˆæœ¬")
    
    # æµ‹è¯•ç”ŸæˆåŠŸèƒ½
    if args.test_generation:
        print(f"\nğŸ§ª æµ‹è¯•é‡åŒ–æ¨¡å‹ç”ŸæˆåŠŸèƒ½")
        model.eval()
        with torch.no_grad():
            try:
                # ç®€å•æµ‹è¯•
                batch_size = args.batch_size
                z = torch.randn(batch_size, 512, device=device)
                z = z.view(batch_size, -1, 1, 1)
                
                velocity, seismic = model.decoder(z)
                print(f"âœ… ç”Ÿæˆæµ‹è¯•æˆåŠŸ:")
                print(f"  é€Ÿåº¦åœº: {velocity.shape}")
                print(f"  åœ°éœ‡æ•°æ®: {seismic.shape}")
                
            except Exception as e:
                print(f"âŒ ç”Ÿæˆæµ‹è¯•å¤±è´¥: {e}")
                return
    
    # åˆ›å»ºç®€åŒ–æ¨¡å‹ç”¨äºéƒ¨ç½²
    print(f"\nğŸ“¦ åˆ›å»ºéƒ¨ç½²æ¨¡å‹")
    deploy_model = SimpleQuantizedModel(
        quantized_diffusion=model.diffusion,
        quantized_decoder=model.decoder
    )
    deploy_model.eval()
    deploy_model = deploy_model.to(device)
    
    # å¯¼å‡ºTorchScript
    print(f"\nğŸ“¤ å¯¼å‡ºé‡åŒ–TorchScriptæ¨¡å‹")
    
    try:
        # åˆ›å»ºè¿½è¸ªè¾“å…¥
        example_input = torch.tensor(50, dtype=torch.int32, device=device)  # noise_steps
        
        # è¿½è¸ªæ¨¡å‹
        with torch.no_grad():
            traced_model = torch.jit.trace(
                deploy_model,
                example_input,
                check_trace=False,
                strict=False
            )
        
        # ä¿å­˜æ¨¡å‹
        output_path = os.path.join(args.output_dir, f"{args.model_name}.pt")
        traced_model.save(output_path)
        
        # æ£€æŸ¥æ–‡ä»¶å¤§å°
        model_size = os.path.getsize(output_path) / (1024 * 1024)
        print(f"âœ… TorchScriptæ¨¡å‹å·²å¯¼å‡º:")
        print(f"  è·¯å¾„: {output_path}")
        print(f"  å¤§å°: {model_size:.2f} MB")
        
        # éªŒè¯ä¿å­˜çš„æ¨¡å‹
        print(f"\nğŸ” éªŒè¯ä¿å­˜çš„TorchScriptæ¨¡å‹")
        loaded_model = torch.jit.load(output_path, map_location=device)
        
        # æ£€æŸ¥é‡åŒ–çŠ¶æ€
        loaded_state = loaded_model.state_dict()
        quantized_after_save = sum(1 for param in loaded_state.values() 
                                 if 'qint' in str(param.dtype) or 'quint' in str(param.dtype))
        
        if quantized_after_save > 0:
            print(f"âœ… TorchScriptä¿å­˜æˆåŠŸä¿ç•™äº† {quantized_after_save} ä¸ªé‡åŒ–å‚æ•°")
            export_success = True
        else:
            print(f"âŒ TorchScriptä¿å­˜ä¸¢å¤±äº†é‡åŒ–ä¿¡æ¯")
            export_success = False
            
    except Exception as e:
        print(f"âŒ TorchScriptå¯¼å‡ºå¤±è´¥: {e}")
        export_success = False
    
    # å¦‚æœTorchScriptå¤±è´¥ï¼Œä¿å­˜ä¸ºæ™®é€šPyTorchæ¨¡å‹
    if not export_success:
        print(f"\nğŸ“¦ å›é€€ï¼šä¿å­˜ä¸ºPyTorché‡åŒ–æ¨¡å‹")
        
        pytorch_path = os.path.join(args.output_dir, f"{args.model_name}_pytorch.pt")
        
        save_dict = {
            'model_state_dict': model.state_dict(),
            'model_class': 'QuantizedUBDiff',
            'quantization_info': {
                'backend': args.backend,
                'is_quantized': True,
                'quantized_params_count': quantized_after_save
            },
            'deployment_info': {
                'input_format': 'noise_steps (int)',
                'output_format': '(velocity, seismic) tensors',
                'recommended_device': 'cpu'
            }
        }
        
        torch.save(save_dict, pytorch_path)
        model_size = os.path.getsize(pytorch_path) / (1024 * 1024)
        print(f"âœ… PyTorché‡åŒ–æ¨¡å‹å·²ä¿å­˜:")
        print(f"  è·¯å¾„: {pytorch_path}")
        print(f"  å¤§å°: {model_size:.2f} MB")
    
    # åˆ›å»ºéƒ¨ç½²æŒ‡å—
    guide_path = os.path.join(args.output_dir, "deployment_guide_int8.md")
    with open(guide_path, 'w', encoding='utf-8') as f:
        f.write("# INT8é‡åŒ–æ¨¡å‹éƒ¨ç½²æŒ‡å—\n\n")
        f.write("## ğŸ¯ æ¨¡å‹ä¿¡æ¯\n\n")
        f.write(f"- æ¨¡å‹ç±»å‹: INT8é‡åŒ–UB-Diff\n")
        f.write(f"- é‡åŒ–åç«¯: {args.backend}\n")
        f.write(f"- æ–‡ä»¶å¤§å°: {model_size:.2f} MB\n")
        f.write(f"- æ¨èè®¾å¤‡: CPU (é‡åŒ–ä¼˜åŒ–)\n\n")
        
        if export_success:
            f.write("## ğŸš€ TorchScriptéƒ¨ç½²\n\n")
            f.write("```python\n")
            f.write("import torch\n\n")
            f.write("# åŠ è½½é‡åŒ–TorchScriptæ¨¡å‹\n")
            f.write(f"model = torch.jit.load('{args.model_name}.pt')\n")
            f.write("model.eval()\n\n")
            f.write("# ç”Ÿæˆæ•°æ®\n")
            f.write("with torch.no_grad():\n")
            f.write("    velocity, seismic = model(50)  # noise_steps=50\n")
            f.write("    print(f'é€Ÿåº¦åœº: {velocity.shape}')\n")
            f.write("    print(f'åœ°éœ‡æ•°æ®: {seismic.shape}')\n")
            f.write("```\n\n")
        else:
            f.write("## ğŸš€ PyTorchéƒ¨ç½²\n\n")
            f.write("```python\n")
            f.write("import torch\n")
            f.write("from qat_deployment.models import QuantizedUBDiff\n\n")
            f.write("# åŠ è½½é‡åŒ–PyTorchæ¨¡å‹\n")
            f.write(f"checkpoint = torch.load('{args.model_name}_pytorch.pt')\n")
            f.write("model_state = checkpoint['model_state_dict']\n")
            f.write("# é‡å»ºæ¨¡å‹å¹¶åŠ è½½æƒé‡...\n")
            f.write("```\n\n")
        
        f.write("## ğŸ“‹ æ€§èƒ½ç‰¹ç‚¹\n\n")
        f.write("- âœ… æ¨¡å‹å¤§å°æ˜¾è‘—å‡å°ï¼ˆçº¦1/4ï¼‰\n")
        f.write("- âœ… CPUæ¨ç†é€Ÿåº¦æå‡\n")
        f.write("- âœ… å†…å­˜ä½¿ç”¨é™ä½\n")
        f.write("- âš ï¸ é‡åŒ–å¯èƒ½ç•¥å¾®å½±å“ç²¾åº¦\n\n")
        
        f.write("## ğŸ› ï¸ æ ‘è“æ´¾éƒ¨ç½²å»ºè®®\n\n")
        f.write("1. ä½¿ç”¨CPUç‰ˆæœ¬PyTorch\n")
        f.write("2. è®¾ç½®æ­£ç¡®çš„é‡åŒ–åç«¯\n")
        f.write("3. å›ºå®šæ‰¹æ¬¡å¤§å°ä¸º1\n")
        f.write("4. é¢„çƒ­æ¨¡å‹ä»¥è·å¾—ç¨³å®šæ€§èƒ½\n")
    
    print(f"ğŸ“– éƒ¨ç½²æŒ‡å—å·²ä¿å­˜åˆ°: {guide_path}")
    
    # æ€»ç»“
    print(f"\nğŸ‰ å¯¼å‡ºå®Œæˆ!")
    if export_success:
        print(f"âœ… æˆåŠŸå¯¼å‡ºINT8é‡åŒ–TorchScriptæ¨¡å‹")
        print(f"âœ… æ¨¡å‹æ”¯æŒæ ‘è“æ´¾éƒ¨ç½²")
        print(f"âœ… æ–‡ä»¶å¤§å°: {model_size:.2f} MB")
    else:
        print(f"âš ï¸ TorchScriptå¯¼å‡ºå¤±è´¥ï¼Œå·²ä¿å­˜PyTorchæ ¼å¼")
        print(f"ğŸ’¡ å»ºè®®ä½¿ç”¨PyTorchæ ¼å¼è¿›è¡Œéƒ¨ç½²")


if __name__ == '__main__':
    main() 