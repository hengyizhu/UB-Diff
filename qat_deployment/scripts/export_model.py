#!/usr/bin/env python3
"""
å¯¼å‡ºé‡åŒ–æ¨¡å‹ä¸ºTorchScriptæ ¼å¼

ç”¨äºæ ‘è“æ´¾éƒ¨ç½²
"""

import os
import sys
import argparse
import torch
import torch.nn as nn
import torch.quantization as quant
from typing import Tuple

# æ·»åŠ é¡¹ç›®è·¯å¾„åˆ°sys.path
current_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.dirname(os.path.dirname(current_dir))
if project_root not in sys.path:
    sys.path.insert(0, project_root)

from qat_deployment.models import (
    QuantizedUBDiff, 
    convert_to_quantized,
    export_quantized_torchscript,
    check_model_quantizable
)


def parse_args():
    parser = argparse.ArgumentParser(description='å¯¼å‡ºé‡åŒ–æ¨¡å‹')
    
    parser.add_argument('--checkpoint_path', type=str, required=True,
                        help='QATæ¨¡å‹æ£€æŸ¥ç‚¹è·¯å¾„')
    parser.add_argument('--output_dir', type=str, default='./exported_models',
                        help='è¾“å‡ºç›®å½•')
    parser.add_argument('--model_name', type=str, default='ub_diff_quantized',
                        help='å¯¼å‡ºæ¨¡å‹åç§°')
    parser.add_argument('--batch_size', type=int, default=1,
                        help='å¯¼å‡ºæ—¶çš„æ‰¹æ¬¡å¤§å°')
    parser.add_argument('--backend', type=str, default='qnnpack',
                        choices=['qnnpack', 'fbgemm'],
                        help='é‡åŒ–åç«¯')
    parser.add_argument('--convert_conv1d', action='store_true',
                        help='æ˜¯å¦è½¬æ¢1Då·ç§¯ä¸º2Då·ç§¯')
    parser.add_argument('--force_quantization', action='store_true',
                        help='æ˜¯å¦å¼ºåˆ¶è¿›è¡Œé‡åŒ–è½¬æ¢ï¼ˆå³ä½¿æœ‰1Då·ç§¯ï¼‰')
    parser.add_argument('--test_generation', action='store_true',
                        help='æ˜¯å¦æµ‹è¯•ç”ŸæˆåŠŸèƒ½')
    parser.add_argument('--optimize_for_mobile', action='store_true',
                        help='æ˜¯å¦ä¸ºç§»åŠ¨è®¾å¤‡ä¼˜åŒ–')
    parser.add_argument('--export_qat_only', action='store_true',
                        help='åªå¯¼å‡ºQATæ¨¡å‹ï¼Œä¸è¿›è¡Œé‡åŒ–è½¬æ¢ï¼ˆæ¨èç”¨äºè§£å†³å…¼å®¹æ€§é—®é¢˜ï¼‰')
    parser.add_argument('--use_cpu_backend', action='store_true',
                        help='å¼ºåˆ¶ä½¿ç”¨CPUå…¼å®¹çš„é‡åŒ–é…ç½®')
    
    return parser.parse_args()


class DeploymentModel(nn.Module):
    """ç”¨äºéƒ¨ç½²çš„ç®€åŒ–æ¨¡å‹
    
    åªåŒ…å«ç”ŸæˆåŠŸèƒ½ï¼Œä¸åŒ…å«è®­ç»ƒç›¸å…³çš„ä»£ç 
    """
    
    def __init__(self, quantized_model: QuantizedUBDiff):
        super(DeploymentModel, self).__init__()
        self.model = quantized_model
        
    def forward(self, batch_size: int = 1) -> Tuple[torch.Tensor, torch.Tensor]:
        """ç”Ÿæˆæ•°æ®
        
        Args:
            batch_size: ç”Ÿæˆçš„æ‰¹æ¬¡å¤§å°
            
        Returns:
            (velocity, seismic): ç”Ÿæˆçš„é€Ÿåº¦åœºå’Œåœ°éœ‡æ•°æ®
        """
        # ä¸ºäº†é¿å…TorchScriptè¿½è¸ªæ—¶æ‰§è¡Œå®Œæ•´æ‰©æ•£è¿‡ç¨‹ï¼Œä½¿ç”¨ç®€åŒ–çš„é‡‡æ ·
        if torch.jit.is_tracing():
            # TorchScriptè¿½è¸ªæ¨¡å¼ï¼šä½¿ç”¨éšæœºæ½œåœ¨è¡¨ç¤º
            device = next(self.model.parameters()).device
            z = torch.randn(batch_size, self.model.encoder_dim, device=device)
            z = z.view(batch_size, -1, 1, 1)
            velocity, seismic = self.model.decoder(z)
            return velocity, seismic
        else:
            # æ­£å¸¸æ¨ç†æ¨¡å¼ï¼šä½¿ç”¨å®Œæ•´çš„ç”Ÿæˆæ–¹æ³•
            velocity, seismic = self.model.generate(batch_size, next(self.model.parameters()).device)
            return velocity, seismic


class QATDeploymentModel(nn.Module):
    """QATæ¨¡å‹éƒ¨ç½²åŒ…è£…å™¨ï¼ˆä¸è¿›è¡Œé‡åŒ–è½¬æ¢ï¼‰"""
    
    def __init__(self, qat_model: QuantizedUBDiff):
        super(QATDeploymentModel, self).__init__()
        self.model = qat_model
        self.encoder_dim = qat_model.encoder_dim
        
    def forward(self, batch_size: int = 1) -> Tuple[torch.Tensor, torch.Tensor]:
        """ç”Ÿæˆæ•°æ®ï¼ˆä½¿ç”¨QATæ¨¡å‹ï¼‰"""
        # ç¡®ä¿æ¨¡å‹åœ¨è¯„ä¼°æ¨¡å¼
        self.model.eval()
        
        # ä½¿ç”¨QATæ¨¡å‹ç”Ÿæˆï¼ˆä¸è½¬æ¢ä¸ºé‡åŒ–æ¨¡å‹ï¼‰
        device = next(self.model.parameters()).device
        
        # ä¸ºäº†é¿å…TorchScriptè¿½è¸ªæ—¶æ‰§è¡Œå®Œæ•´æ‰©æ•£è¿‡ç¨‹ï¼Œä½¿ç”¨ç®€åŒ–çš„é‡‡æ ·
        # åœ¨å®é™…æ¨ç†æ—¶ï¼Œå¯ä»¥æ›¿æ¢ä¸ºå®Œæ•´çš„æ‰©æ•£é‡‡æ ·
        if torch.jit.is_tracing():
            # TorchScriptè¿½è¸ªæ¨¡å¼ï¼šä½¿ç”¨éšæœºæ½œåœ¨è¡¨ç¤º
            z = torch.randn(batch_size, self.encoder_dim, device=device)
        else:
            # æ­£å¸¸æ¨ç†æ¨¡å¼ï¼šä½¿ç”¨æ‰©æ•£é‡‡æ ·
            try:
                z = self.model.sample_latent(batch_size, device)
            except Exception as e:
                print(f"âš ï¸ æ‰©æ•£é‡‡æ ·å¤±è´¥ï¼Œä½¿ç”¨éšæœºé‡‡æ ·: {e}")
                z = torch.randn(batch_size, self.encoder_dim, device=device)
        
        # è§£ç 
        z = z.view(z.shape[0], -1, 1, 1)
        velocity, seismic = self.model.decoder(z)
        
        return velocity, seismic


def main():
    args = parse_args()
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(args.output_dir, exist_ok=True)
    
    # æ£€æŸ¥PyTorchç‰ˆæœ¬å’Œé‡åŒ–æ”¯æŒ
    print(f"PyTorchç‰ˆæœ¬: {torch.__version__}")
    print(f"é‡åŒ–æ”¯æŒ: {torch.backends.quantized.supported_engines}")
    
    # è®¾ç½®é‡åŒ–åç«¯
    if args.use_cpu_backend or args.backend not in torch.backends.quantized.supported_engines:
        # ä½¿ç”¨CPUå…¼å®¹çš„åç«¯
        available_backends = torch.backends.quantized.supported_engines
        if 'fbgemm' in available_backends:
            backend = 'fbgemm'
        elif 'qnnpack' in available_backends:
            backend = 'qnnpack'
        else:
            print("âš ï¸ è­¦å‘Šï¼šæ²¡æœ‰æ‰¾åˆ°æ”¯æŒçš„é‡åŒ–åç«¯ï¼Œå°†ä½¿ç”¨QATæ¨¡å¼")
            args.export_qat_only = True
            backend = args.backend
        
        if backend != args.backend:
            print(f"âš ï¸ åç«¯ä» {args.backend} åˆ‡æ¢åˆ° {backend} ä»¥æé«˜å…¼å®¹æ€§")
    else:
        backend = args.backend
    
    torch.backends.quantized.engine = backend
    print(f"ä½¿ç”¨é‡åŒ–åç«¯: {backend}")
    
    # åŠ è½½QATæ¨¡å‹
    print(f"åŠ è½½QATæ¨¡å‹: {args.checkpoint_path}")
    checkpoint = torch.load(args.checkpoint_path, map_location='cpu')
    
    # åˆ›å»ºæ¨¡å‹
    print("=== åˆ›å»ºæ”¹è¿›çš„é‡åŒ–æ¨¡å‹ ===")
    model = QuantizedUBDiff(
        encoder_dim=512,
        velocity_channels=1,
        seismic_channels=5,
        dim_mults=(1, 2, 4, 8),
        time_steps=256,
        quantize_diffusion=True,
        quantize_decoder=True
    )
    
    # åº”ç”¨æ”¹è¿›çš„é‡åŒ–ç­–ç•¥ï¼ˆé‡å»ºè®­ç»ƒæ—¶çš„é…ç½®ï¼‰
    print("é‡å»ºé‡åŒ–é…ç½®...")
    try:
        quantization_report = model.apply_improved_quantization(
            backend=backend,
            convert_conv1d=args.convert_conv1d,
            use_aggressive_config=True  # å‡è®¾è®­ç»ƒæ—¶ä½¿ç”¨äº†æ¿€è¿›é…ç½®
        )
    except Exception as e:
        print(f"âš ï¸ é‡åŒ–é…ç½®åº”ç”¨å¤±è´¥: {e}")
        print("å›é€€åˆ°åŸºç¡€é…ç½®...")
        # ä½¿ç”¨åŸºç¡€é…ç½®
        quantization_report = {
            'total_modules': 0,
            'quantizable_modules': 0,
            'quantizable_ratio': 0,
            'conv1d_count': 0,
            'converted_conv1d': 0
        }
        args.export_qat_only = True  # å¼ºåˆ¶ä½¿ç”¨QATæ¨¡å¼
    
    # åŠ è½½æƒé‡
    print("åŠ è½½è®­ç»ƒåçš„æƒé‡...")
    try:
        if 'model_state_dict' in checkpoint:
            model.load_state_dict(checkpoint['model_state_dict'], strict=False)
        else:
            model.load_state_dict(checkpoint, strict=False)
        print("âœ“ æƒé‡åŠ è½½æˆåŠŸï¼ˆå…è®¸éƒ¨åˆ†é”®ä¸åŒ¹é…ï¼‰")
    except Exception as e:
        print(f"âš ï¸ ç›´æ¥åŠ è½½å¤±è´¥: {e}")
        print("å°è¯•è¿‡æ»¤é‡åŒ–ç›¸å…³é”®åé‡æ–°åŠ è½½...")
        
        # è·å–state_dict
        if 'model_state_dict' in checkpoint:
            saved_state_dict = checkpoint['model_state_dict']
        else:
            saved_state_dict = checkpoint
        
        # è¿‡æ»¤æ‰é‡åŒ–ç›¸å…³çš„é”®ï¼Œåªä¿ç•™å®é™…çš„æ¨¡å‹å‚æ•°
        filtered_state_dict = {}
        skipped_keys = []
        
        for key, value in saved_state_dict.items():
            # è·³è¿‡é‡åŒ–ç›¸å…³çš„é”®
            if any(pattern in key for pattern in [
                'activation_post_process', 'fake_quant', 'weight_fake_quant',
                'observer_enabled', 'fake_quant_enabled', 'scale', 'zero_point'
            ]):
                skipped_keys.append(key)
                continue
            filtered_state_dict[key] = value
        
        print(f"è¿‡æ»¤æ‰ {len(skipped_keys)} ä¸ªé‡åŒ–ç›¸å…³é”®")
        print(f"ä¿ç•™ {len(filtered_state_dict)} ä¸ªæ¨¡å‹å‚æ•°é”®")
        
        # å°è¯•åŠ è½½è¿‡æ»¤åçš„æƒé‡
        try:
            model.load_state_dict(filtered_state_dict, strict=False)
            print("âœ“ è¿‡æ»¤åŠ è½½æˆåŠŸ")
        except Exception as e2:
            print(f"âš ï¸ è¿‡æ»¤åŠ è½½ä¹Ÿå¤±è´¥: {e2}")
            print("å°è¯•é€ä¸ªåŠ è½½å…¼å®¹çš„é”®...")
            
            model_keys = set(model.state_dict().keys())
            compatible_keys = set(filtered_state_dict.keys()) & model_keys
            incompatible_keys = set(filtered_state_dict.keys()) - model_keys
            
            print(f"å…¼å®¹é”®æ•°é‡: {len(compatible_keys)}")
            print(f"ä¸å…¼å®¹é”®æ•°é‡: {len(incompatible_keys)}")
            
            if incompatible_keys:
                print("ä¸å…¼å®¹çš„é”®ï¼ˆå‰10ä¸ªï¼‰:")
                for key in list(incompatible_keys)[:10]:
                    print(f"  {key}")
            
            # åªåŠ è½½å…¼å®¹çš„é”®
            compatible_state_dict = {k: v for k, v in filtered_state_dict.items() if k in compatible_keys}
            model.load_state_dict(compatible_state_dict, strict=False)
            print(f"âœ“ æˆåŠŸåŠ è½½ {len(compatible_state_dict)} ä¸ªå…¼å®¹å‚æ•°")
    
    model.eval()
    
    # æ˜¾ç¤ºé‡åŒ–ä¿¡æ¯
    print(f"\n=== é‡åŒ–æ¨¡å‹ä¿¡æ¯ ===")
    print(f"æ€»æ¨¡å—æ•°: {quantization_report['total_modules']}")
    print(f"å¯é‡åŒ–æ¨¡å—æ•°: {quantization_report['quantizable_modules']}")
    print(f"é‡åŒ–ç‡: {quantization_report['quantizable_ratio']:.2%}")
    print(f"1Då·ç§¯æ•°: {quantization_report['conv1d_count']}")
    if quantization_report['converted_conv1d'] > 0:
        print(f"âœ“ å·²è½¬æ¢ {quantization_report['converted_conv1d']} ä¸ª1Då·ç§¯å±‚")
    
    # å†³å®šå¯¼å‡ºç­–ç•¥ - é»˜è®¤ä½¿ç”¨QATæ¨¡å¼é¿å…å…¼å®¹æ€§é—®é¢˜
    if args.export_qat_only or not args.force_quantization:
        print("\n=== å¯¼å‡ºQATæ¨¡å‹ï¼ˆä¸è¿›è¡Œé‡åŒ–è½¬æ¢ï¼‰===")
        print("ğŸ’¡ ä½¿ç”¨QATæ¨¡å¼å¯ä»¥é¿å…é‡åŒ–å…¼å®¹æ€§é—®é¢˜")
        deployment_model = QATDeploymentModel(model)
        model_type = "QAT"
    else:
        # å†³å®šæ˜¯å¦è¿›è¡Œæœ€ç»ˆçš„é‡åŒ–è½¬æ¢
        should_convert = args.force_quantization or quantization_report['conv1d_count'] == 0
        
        if should_convert:
            print("\n=== è½¬æ¢ä¸ºå®Œå…¨é‡åŒ–æ¨¡å‹ ===")
            print("âš ï¸ è­¦å‘Šï¼šé‡åŒ–è½¬æ¢å¯èƒ½å­˜åœ¨å…¼å®¹æ€§é—®é¢˜")
            try:
                # åœ¨è½¬æ¢å‰ç¡®ä¿æ¨¡å‹åœ¨CPUä¸Š
                model = model.cpu()
                model = model.convert_to_quantized()
                print("âœ“ é‡åŒ–è½¬æ¢æˆåŠŸ")
                deployment_model = DeploymentModel(model)
                model_type = "Quantized"
            except Exception as e:
                print(f"âš ï¸ é‡åŒ–è½¬æ¢å¤±è´¥: {e}")
                print("å›é€€åˆ°QATæ¨¡å‹å¯¼å‡º")
                deployment_model = QATDeploymentModel(model)
                model_type = "QAT"
        else:
            print(f"\nâš ï¸ è·³è¿‡é‡åŒ–è½¬æ¢ï¼ˆå­˜åœ¨ {quantization_report['conv1d_count']} ä¸ª1Då·ç§¯ï¼‰")
            print("ä½¿ç”¨QATæ¨¡å‹è¿›è¡Œå¯¼å‡º")
            deployment_model = QATDeploymentModel(model)
            model_type = "QAT"
    
    # ç¡®ä¿éƒ¨ç½²æ¨¡å‹åœ¨CPUä¸Š
    deployment_model = deployment_model.cpu()
    deployment_model.eval()
    
    # æµ‹è¯•ç”ŸæˆåŠŸèƒ½
    if args.test_generation:
        print(f"\næµ‹è¯•ç”ŸæˆåŠŸèƒ½ï¼ˆ{model_type}æ¨¡å‹ï¼‰...")
        with torch.no_grad():
            try:
                velocity, seismic = deployment_model(batch_size=args.batch_size)
                print(f"ç”Ÿæˆé€Ÿåº¦åœºå½¢çŠ¶: {velocity.shape}")
                print(f"ç”Ÿæˆåœ°éœ‡æ•°æ®å½¢çŠ¶: {seismic.shape}")
                print("âœ“ ç”Ÿæˆæµ‹è¯•æˆåŠŸï¼")
            except Exception as e:
                print(f"âš ï¸ ç”Ÿæˆæµ‹è¯•å¤±è´¥: {e}")
                print("ç»§ç»­å¯¼å‡ºè¿‡ç¨‹...")
    
    # å¯¼å‡ºTorchScript
    print(f"\nå¯¼å‡ºTorchScriptæ¨¡å‹ï¼ˆ{model_type}ï¼‰...")
    
    # åˆ›å»ºç¤ºä¾‹è¾“å…¥
    example_batch_size = torch.tensor(args.batch_size, dtype=torch.int32)
    
    try:
        # è¿½è¸ªæ¨¡å‹
        with torch.no_grad():
            traced_model = torch.jit.trace(
                deployment_model,
                example_batch_size,
                check_trace=False,
                strict=False  # å…è®¸ä¸€äº›ä¸ä¸¥æ ¼çš„è¿½è¸ª
            )
        
        # ä¼˜åŒ–æ¨¡å‹
        if args.optimize_for_mobile:
            print("ä¸ºç§»åŠ¨è®¾å¤‡ä¼˜åŒ–æ¨¡å‹...")
            # æ£€æŸ¥æ˜¯å¦æ”¯æŒç§»åŠ¨è®¾å¤‡ä¼˜åŒ–
            if hasattr(torch.jit, 'optimize_for_mobile'):
                traced_model = torch.jit.optimize_for_mobile(traced_model)
                print("ç§»åŠ¨è®¾å¤‡ä¼˜åŒ–å®Œæˆ")
            else:
                print("å½“å‰PyTorchç‰ˆæœ¬ä¸æ”¯æŒoptimize_for_mobileï¼Œè·³è¿‡ç§»åŠ¨è®¾å¤‡ä¼˜åŒ–")
        
        # ä¿å­˜æ¨¡å‹
        output_path = os.path.join(args.output_dir, f"{args.model_name}.pt")
        traced_model.save(output_path)
        print(f"âœ“ æ¨¡å‹å·²å¯¼å‡ºåˆ°: {output_path}")
        
        # è®¡ç®—æ¨¡å‹å¤§å°
        model_size = os.path.getsize(output_path) / (1024 * 1024)  # MB
        print(f"æ¨¡å‹å¤§å°: {model_size:.2f} MB")
        
        export_success = True
        
    except Exception as e:
        print(f"âš ï¸ TorchScriptå¯¼å‡ºå¤±è´¥: {e}")
        print("å°è¯•ä¿å­˜PyTorchæ¨¡å‹...")
        
        # å›é€€ï¼šä¿å­˜ä¸ºæ™®é€šPyTorchæ¨¡å‹
        output_path = os.path.join(args.output_dir, f"{args.model_name}_pytorch.pt")
        torch.save({
            'model': deployment_model.state_dict(),
            'model_type': model_type,
            'quantization_report': quantization_report
        }, output_path)
        
        model_size = os.path.getsize(output_path) / (1024 * 1024)  # MB
        print(f"PyTorchæ¨¡å‹å·²ä¿å­˜åˆ°: {output_path}")
        print(f"æ¨¡å‹å¤§å°: {model_size:.2f} MB")
        
        export_success = False
    
    # ä¿å­˜æ¨¡å‹ä¿¡æ¯
    info_path = os.path.join(args.output_dir, f"{args.model_name}_info.txt")
    with open(info_path, 'w') as f:
        f.write(f"æ¨¡å‹åç§°: {args.model_name}\n")
        f.write(f"æ¨¡å‹ç±»å‹: {model_type}\n")
        f.write(f"é‡åŒ–åç«¯: {backend}\n")
        f.write(f"æ¨¡å‹å¤§å°: {model_size:.2f} MB\n")
        f.write(f"æ‰¹æ¬¡å¤§å°: {args.batch_size}\n")
        f.write(f"ç¼–ç å™¨ç»´åº¦: 512\n")
        f.write(f"é€Ÿåº¦åœºè¾“å‡º: (1, 70, 70)\n")
        f.write(f"åœ°éœ‡æ•°æ®è¾“å‡º: (5, 1000, 70)\n")
        f.write(f"å¯¼å‡ºæ ¼å¼: {'TorchScript' if export_success else 'PyTorch'}\n")
        f.write(f"\né‡åŒ–ä¿¡æ¯:\n")
        f.write(f"æ€»æ¨¡å—æ•°: {quantization_report['total_modules']}\n")
        f.write(f"å¯é‡åŒ–æ¨¡å—æ•°: {quantization_report['quantizable_modules']}\n")
        f.write(f"é‡åŒ–ç‡: {quantization_report['quantizable_ratio']:.2%}\n")
        f.write(f"1Då·ç§¯æ•°: {quantization_report['conv1d_count']}\n")
        if quantization_report['converted_conv1d'] > 0:
            f.write(f"âœ“ å·²è½¬æ¢ {quantization_report['converted_conv1d']} ä¸ª1Då·ç§¯å±‚\n")
    
    print(f"æ¨¡å‹ä¿¡æ¯å·²ä¿å­˜åˆ°: {info_path}")
    
    # åˆ›å»ºéƒ¨ç½²æŒ‡å—
    deploy_guide_path = os.path.join(args.output_dir, "deployment_guide.md")
    with open(deploy_guide_path, 'w') as f:
        f.write("# æ ‘è“æ´¾éƒ¨ç½²æŒ‡å—\n\n")
        f.write("## 1. ç¯å¢ƒå‡†å¤‡\n\n")
        f.write("```bash\n")
        f.write("# å®‰è£…PyTorch (æ ‘è“æ´¾ç‰ˆæœ¬)\n")
        f.write("pip install torch==1.13.0\n")
        f.write("```\n\n")
        f.write("## 2. åŠ è½½æ¨¡å‹\n\n")
        
        if export_success:
            f.write("```python\n")
            f.write("import torch\n\n")
            f.write("# åŠ è½½TorchScriptæ¨¡å‹\n")
            f.write(f"model = torch.jit.load('{args.model_name}.pt')\n")
            f.write("model.eval()\n\n")
            f.write("# ç”Ÿæˆæ•°æ®\n")
            f.write("with torch.no_grad():\n")
            f.write("    velocity, seismic = model(1)  # batch_size=1\n")
            f.write("```\n\n")
        else:
            f.write("```python\n")
            f.write("import torch\n")
            f.write("from your_model_module import QATDeploymentModel, QuantizedUBDiff\n\n")
            f.write("# åŠ è½½PyTorchæ¨¡å‹\n")
            f.write(f"checkpoint = torch.load('{args.model_name}_pytorch.pt')\n")
            f.write("# é‡å»ºæ¨¡å‹å¹¶åŠ è½½æƒé‡\n")
            f.write("# model = create_model_and_load_weights(checkpoint)\n")
            f.write("```\n\n")
        
        f.write("## 3. æ€§èƒ½ä¼˜åŒ–\n\n")
        f.write("- ä½¿ç”¨å•æ‰¹æ¬¡æ¨ç†ä»¥å‡å°‘å†…å­˜ä½¿ç”¨\n")
        f.write("- è€ƒè™‘ä½¿ç”¨åŠç²¾åº¦ï¼ˆfp16ï¼‰è¿›ä¸€æ­¥å‡å°‘å†…å­˜\n")
        f.write("- å¯ä»¥è°ƒæ•´æ‰©æ•£æ­¥æ•°ä»¥åŠ å¿«ç”Ÿæˆé€Ÿåº¦\n")
        f.write(f"- å½“å‰æ¨¡å‹ç±»å‹: {model_type}\n")
        if model_type == "QAT":
            f.write("- QATæ¨¡å‹ä¿ç•™äº†é‡åŒ–æ„ŸçŸ¥è®­ç»ƒçš„ä¼˜åŒ–ï¼Œä½†æœªå®Œå…¨è½¬æ¢ä¸ºINT8\n")
            f.write("- é¿å…äº†é‡åŒ–å…¼å®¹æ€§é—®é¢˜ï¼Œæ¨èç”¨äºç”Ÿäº§ç¯å¢ƒ\n")
        else:
            f.write("- é‡åŒ–æ¨¡å‹å·²å®Œå…¨è½¬æ¢ä¸ºINT8ï¼Œå…·æœ‰æœ€ä½³çš„æ¨ç†æ€§èƒ½\n")
        
        f.write("\n## 4. æ•…éšœæ’é™¤\n\n")
        f.write("å¦‚æœé‡åˆ°é‡åŒ–ç›¸å…³é”™è¯¯ï¼Œå»ºè®®ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤é‡æ–°å¯¼å‡ºï¼š\n\n")
        f.write("```bash\n")
        f.write("python qat_deployment/scripts/export_model.py \\\n")
        f.write("    --checkpoint_path \"./checkpoints/qat_diffusion/final_qat_model.pt\" \\\n")
        f.write("    --export_qat_only \\\n")
        f.write("    --test_generation\n")
        f.write("```\n")
    
    print(f"éƒ¨ç½²æŒ‡å—å·²ä¿å­˜åˆ°: {deploy_guide_path}")
    
    print(f"\nğŸ‰ å¯¼å‡ºå®Œæˆï¼")
    print(f"æ¨¡å‹ç±»å‹: {model_type}")
    print(f"å¯¼å‡ºæ ¼å¼: {'TorchScript' if export_success else 'PyTorch'}")
    print(f"æ¨¡å‹å¤§å°: {model_size:.2f} MB")
    
    # ç»™å‡ºå»ºè®®
    if model_type == "QAT":
        print("\nğŸ’¡ å»ºè®®ï¼š")
        print("- QATæ¨¡å‹é¿å…äº†é‡åŒ–å…¼å®¹æ€§é—®é¢˜ï¼Œæ¨èç”¨äºç”Ÿäº§ç¯å¢ƒ")
        print("- å¦‚éœ€æ›´å°çš„æ¨¡å‹ï¼Œå¯ä»¥å°è¯•æ·»åŠ  --force_quantization å‚æ•°")
    elif not export_success:
        print("\nğŸ’¡ å»ºè®®ï¼š")
        print("- å¦‚æœTorchScriptå¯¼å‡ºå¤±è´¥ï¼Œå¯ä»¥ä½¿ç”¨PyTorchæ ¼å¼")
        print("- å»ºè®®ä½¿ç”¨ --export_qat_only å‚æ•°é¿å…å…¼å®¹æ€§é—®é¢˜")


if __name__ == '__main__':
    main() 