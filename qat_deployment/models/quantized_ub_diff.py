"""
é‡åŒ–ç‰ˆæœ¬çš„UB-Diffæ¨¡å‹

ä¸“é—¨ä¸ºæ ‘è“æ´¾éƒ¨ç½²ä¼˜åŒ–çš„é‡åŒ–æ¨¡å‹ï¼Œé›†æˆæ”¹è¿›çš„é‡åŒ–ç­–ç•¥
"""

import torch
import torch.nn as nn
import torch.quantization as quant
from typing import Optional, Tuple, Dict, Any
import sys
import os

# æ·»åŠ é¡¹ç›®è·¯å¾„åˆ°sys.path
current_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.dirname(os.path.dirname(current_dir))
if project_root not in sys.path:
    sys.path.insert(0, project_root)

from model.components import (
    VelocityDecoder, 
    SeismicDecoder,
    Unet1D,
    GaussianDiffusion1DDefault,
    cosine_beta_schedule
)
from model.components.decoder import LatentProjector

# å¯¼å…¥æ”¹è¿›çš„é‡åŒ–åŠŸèƒ½
from .improved_quantization import (
    analyze_model_quantizability,
    apply_improved_quantization,
    create_quantization_report,
    ImprovedQuantizationConfig
)


class Conv1DWrapper(nn.Module):
    """1Då·ç§¯åŒ…è£…å™¨ï¼Œè½¬æ¢ä¸º2Då·ç§¯ä»¥è·å¾—æ›´å¥½çš„é‡åŒ–æ”¯æŒ"""
    
    def __init__(self, conv1d_layer):
        super().__init__()
        # ä¿å­˜åŸå§‹å‚æ•°
        in_channels = conv1d_layer.in_channels
        out_channels = conv1d_layer.out_channels
        kernel_size = conv1d_layer.kernel_size[0]
        stride = conv1d_layer.stride[0]
        padding = conv1d_layer.padding[0]
        dilation = conv1d_layer.dilation[0]
        groups = conv1d_layer.groups
        bias = conv1d_layer.bias is not None
        
        # åˆ›å»ºç­‰æ•ˆçš„2Då·ç§¯
        self.conv2d = nn.Conv2d(
            in_channels=in_channels,
            out_channels=out_channels,
            kernel_size=(1, kernel_size),
            stride=(1, stride),
            padding=(0, padding),
            dilation=(1, dilation),
            groups=groups,
            bias=bias
        )
        
        # å¤åˆ¶æƒé‡
        with torch.no_grad():
            # æƒé‡: (out, in, k) -> (out, in, 1, k)
            self.conv2d.weight.copy_(conv1d_layer.weight.unsqueeze(2))
            if bias:
                self.conv2d.bias.copy_(conv1d_layer.bias)
    
    def forward(self, x):
        # è¾“å…¥: (B, C, L) -> (B, C, 1, L)
        if x.dim() == 3:
            x = x.unsqueeze(2)
        
        # 2Då·ç§¯
        x = self.conv2d(x)
        
        # è¾“å‡º: (B, C, 1, L) -> (B, C, L)
        return x.squeeze(2)


def convert_conv1d_to_conv2d(module):
    """é€’å½’è½¬æ¢æ¨¡å—ä¸­çš„æ‰€æœ‰1Då·ç§¯ä¸º2Då·ç§¯"""
    converted_count = 0
    
    for name, child in list(module.named_children()):
        if isinstance(child, nn.Conv1d):
            # è½¬æ¢1Då·ç§¯
            setattr(module, name, Conv1DWrapper(child))
            converted_count += 1
        else:
            # é€’å½’å¤„ç†å­æ¨¡å—
            converted_count += convert_conv1d_to_conv2d(child)
    
    return converted_count


class QuantizedDecoder(nn.Module):
    """é‡åŒ–çš„è§£ç å™¨æ¨¡å—
    
    åŒ…å«é€Ÿåº¦è§£ç å™¨å’Œåœ°éœ‡è§£ç å™¨çš„é‡åŒ–ç‰ˆæœ¬
    """
    
    def __init__(self, 
                 encoder_dim: int = 512,
                 velocity_channels: int = 1,
                 seismic_channels: int = 5,
                 velocity_latent_dim: int = 128,
                 seismic_latent_dim: int = 640,
                 seismic_h: int = 1000,
                 seismic_w: int = 70,
                 quantize_velocity: bool = True,
                 quantize_seismic: bool = True):
        """
        Args:
            encoder_dim: ç¼–ç å™¨è¾“å‡ºç»´åº¦
            velocity_channels: é€Ÿåº¦åœºé€šé“æ•°
            seismic_channels: åœ°éœ‡æ•°æ®é€šé“æ•°
            velocity_latent_dim: é€Ÿåº¦è§£ç å™¨æ½œåœ¨ç»´åº¦
            seismic_latent_dim: åœ°éœ‡è§£ç å™¨æ½œåœ¨ç»´åº¦
            seismic_h, seismic_w: åœ°éœ‡æ•°æ®å°ºå¯¸
            quantize_velocity: æ˜¯å¦é‡åŒ–é€Ÿåº¦è§£ç å™¨
            quantize_seismic: æ˜¯å¦é‡åŒ–åœ°éœ‡è§£ç å™¨
        """
        super(QuantizedDecoder, self).__init__()
        
        self.quantize_velocity = quantize_velocity
        self.quantize_seismic = quantize_seismic
        
        # ä¸ºæ¯ä¸ªè·¯å¾„åˆ›å»ºç‹¬ç«‹çš„é‡åŒ–å±‚
        self.velocity_quant = quant.QuantStub()
        self.velocity_dequant = quant.DeQuantStub()
        self.seismic_quant = quant.QuantStub()
        self.seismic_dequant = quant.DeQuantStub()
        
        # æ½œåœ¨ç©ºé—´æŠ•å½±å™¨
        self.velocity_projector = LatentProjector(encoder_dim, velocity_latent_dim)
        self.seismic_projector = LatentProjector(encoder_dim, seismic_latent_dim)
        
        # è§£ç å™¨
        self.velocity_decoder = VelocityDecoder(
            latent_dim=velocity_latent_dim,
            out_channels=velocity_channels
        )
        self.seismic_decoder = SeismicDecoder(
            latent_dim=seismic_latent_dim,
            out_channels=seismic_channels,
            origin_h=seismic_h,
            origin_w=seismic_w
        )
        
    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:
        """å‰å‘ä¼ æ’­
        
        Args:
            x: æ½œåœ¨è¡¨ç¤º (B, encoder_dim, 1, 1)
            
        Returns:
            (velocity, seismic): è§£ç åçš„é€Ÿåº¦åœºå’Œåœ°éœ‡æ•°æ®
        """
        # é€Ÿåº¦è§£ç è·¯å¾„
        if self.quantize_velocity:
            x_v = self.velocity_quant(x)
            z_v = self.velocity_projector(x_v)
            velocity = self.velocity_decoder(z_v)
            velocity = self.velocity_dequant(velocity)
        else:
            z_v = self.velocity_projector(x)
            velocity = self.velocity_decoder(z_v)
        
        # åœ°éœ‡è§£ç è·¯å¾„  
        if self.quantize_seismic:
            x_s = self.seismic_quant(x)
            z_s = self.seismic_projector(x_s)
            seismic = self.seismic_decoder(z_s)
            seismic = self.seismic_dequant(seismic)
        else:
            z_s = self.seismic_projector(x)
            seismic = self.seismic_decoder(z_s)
        
        return velocity, seismic
    
    def freeze_velocity_path(self) -> None:
        """å†»ç»“é€Ÿåº¦è§£ç è·¯å¾„"""
        for param in self.velocity_projector.parameters():
            param.requires_grad = False
        for param in self.velocity_decoder.parameters():
            param.requires_grad = False
        print("é€Ÿåº¦è§£ç è·¯å¾„å·²å†»ç»“")
    
    def freeze_seismic_path(self) -> None:
        """å†»ç»“åœ°éœ‡è§£ç è·¯å¾„"""
        for param in self.seismic_projector.parameters():
            param.requires_grad = False
        for param in self.seismic_decoder.parameters():
            param.requires_grad = False
        print("åœ°éœ‡è§£ç è·¯å¾„å·²å†»ç»“")
    
    def unfreeze_velocity_path(self) -> None:
        """è§£å†»é€Ÿåº¦è§£ç è·¯å¾„"""
        for param in self.velocity_projector.parameters():
            param.requires_grad = True
        for param in self.velocity_decoder.parameters():
            param.requires_grad = True
        print("é€Ÿåº¦è§£ç è·¯å¾„å·²è§£å†»")
    
    def unfreeze_seismic_path(self) -> None:
        """è§£å†»åœ°éœ‡è§£ç è·¯å¾„"""
        for param in self.seismic_projector.parameters():
            param.requires_grad = True
        for param in self.seismic_decoder.parameters():
            param.requires_grad = True
        print("åœ°éœ‡è§£ç è·¯å¾„å·²è§£å†»")


class QuantizedUBDiff(nn.Module):
    """é‡åŒ–ç‰ˆæœ¬çš„UB-Diffæ¨¡å‹
    
    ç”¨äºæ ‘è“æ´¾éƒ¨ç½²çš„çº¯ç”Ÿæˆæ¨¡å‹ï¼ˆä¸åŒ…å«ç¼–ç å™¨ï¼‰
    """
    
    def __init__(self,
                 encoder_dim: int = 512,
                 velocity_channels: int = 1,
                 seismic_channels: int = 5,
                 dim_mults: Tuple[int, ...] = (1, 2, 4, 8),
                 time_steps: int = 256,
                 time_scale: int = 1,
                 objective: str = 'pred_v',
                 quantize_diffusion: bool = True,
                 quantize_decoder: bool = True):
        """
        Args:
            encoder_dim: æ½œåœ¨ç©ºé—´ç»´åº¦
            velocity_channels: é€Ÿåº¦åœºé€šé“æ•°
            seismic_channels: åœ°éœ‡æ•°æ®é€šé“æ•°
            dim_mults: U-Netç»´åº¦å€æ•°
            time_steps: æ‰©æ•£æ—¶é—´æ­¥æ•°
            time_scale: æ—¶é—´ç¼©æ”¾å› å­
            objective: æ‰©æ•£ç›®æ ‡å‡½æ•°
            quantize_diffusion: æ˜¯å¦é‡åŒ–æ‰©æ•£æ¨¡å‹
            quantize_decoder: æ˜¯å¦é‡åŒ–è§£ç å™¨
        """
        super(QuantizedUBDiff, self).__init__()
        
        self.encoder_dim = encoder_dim
        self.quantize_diffusion = quantize_diffusion
        self.quantize_decoder = quantize_decoder
        
        # é‡åŒ–/åé‡åŒ–å±‚
        self.quant = quant.QuantStub()
        self.dequant = quant.DeQuantStub()
        
        # 1D U-Netç”¨äºæ‰©æ•£
        self.unet = Unet1D(
            dim=encoder_dim,
            channels=1,
            dim_mults=dim_mults
        )
        
        # æ‰©æ•£è¿‡ç¨‹
        betas = cosine_beta_schedule(timesteps=time_steps)
        self.diffusion = GaussianDiffusion1DDefault(
            model=self.unet,
            seq_length=encoder_dim,
            betas=betas,
            time_scale=time_scale,
            objective=objective,
            use_wandb=False
        )
        
        # é‡åŒ–è§£ç å™¨
        self.decoder = QuantizedDecoder(
            encoder_dim=encoder_dim,
            velocity_channels=velocity_channels,
            seismic_channels=seismic_channels,
            quantize_velocity=quantize_decoder,
            quantize_seismic=quantize_decoder
        )
        
    def sample_latent(self, batch_size: int, device: torch.device) -> torch.Tensor:
        """ä»æ‰©æ•£è¿‡ç¨‹é‡‡æ ·æ½œåœ¨è¡¨ç¤º
        
        Args:
            batch_size: æ‰¹æ¬¡å¤§å°
            device: è®¾å¤‡
            
        Returns:
            é‡‡æ ·çš„æ½œåœ¨è¡¨ç¤º (B, encoder_dim)
        """
        # ä»æ‰©æ•£æ¨¡å‹é‡‡æ ·
        z = self.diffusion.sample(batch_size)  # (B, 1, encoder_dim)
        z = z.squeeze(1)  # (B, encoder_dim)
        return z
    
    def generate(self, batch_size: int, device: torch.device) -> Tuple[torch.Tensor, torch.Tensor]:
        """ç”Ÿæˆæ–°çš„é€Ÿåº¦åœºå’Œåœ°éœ‡æ•°æ®
        
        Args:
            batch_size: æ‰¹æ¬¡å¤§å°
            device: è®¾å¤‡
            
        Returns:
            (velocity, seismic): ç”Ÿæˆçš„é€Ÿåº¦åœºå’Œåœ°éœ‡æ•°æ®
        """
        # é‡‡æ ·æ½œåœ¨è¡¨ç¤º
        z = self.sample_latent(batch_size, device)
        
        # é‡å¡‘ä¸ºè§£ç å™¨æœŸæœ›çš„æ ¼å¼
        z = z.view(batch_size, -1, 1, 1)
        
        # è§£ç 
        velocity, seismic = self.decoder(z)
        
        return velocity, seismic
    
    def forward(self, z: Optional[torch.Tensor] = None, 
                batch_size: Optional[int] = None) -> Tuple[torch.Tensor, torch.Tensor]:
        """å‰å‘ä¼ æ’­ï¼ˆç”¨äºå¯¼å‡ºï¼‰
        
        Args:
            z: å¯é€‰çš„æ½œåœ¨è¡¨ç¤ºè¾“å…¥
            batch_size: å¦‚æœzä¸ºNoneï¼Œç”Ÿæˆçš„æ‰¹æ¬¡å¤§å°
            
        Returns:
            (velocity, seismic): ç”Ÿæˆçš„æ•°æ®
        """
        if z is None:
            if batch_size is None:
                raise ValueError("å¿…é¡»æä¾›zæˆ–batch_size")
            device = next(self.parameters()).device
            
            # ä½¿ç”¨æ‰©æ•£æ¨¡å‹é‡‡æ ·
            z = self.diffusion.sample(batch_size)  # ä½¿ç”¨å†…ç½®é‡‡æ ·æ–¹æ³•
            z = z.squeeze(1) if z.dim() > 2 else z
        
        # é‡å¡‘å¹¶è§£ç 
        z = z.view(z.shape[0], -1, 1, 1)
        velocity, seismic = self.decoder(z)
        
        return velocity, seismic
    
    def load_pretrained_weights(self, checkpoint_path: str) -> None:
        """ä»é¢„è®­ç»ƒæ¨¡å‹åŠ è½½æƒé‡ï¼ˆæ”¹è¿›ç‰ˆæœ¬ï¼‰
        
        Args:
            checkpoint_path: æ£€æŸ¥ç‚¹è·¯å¾„
        """
        print(f"åŠ è½½é¢„è®­ç»ƒæƒé‡: {checkpoint_path}")
        checkpoint = torch.load(checkpoint_path, map_location='cpu', weights_only=False)
        
        # å¤„ç†ä¸åŒçš„æ£€æŸ¥ç‚¹æ ¼å¼
        if 'model' in checkpoint:
            state_dict = checkpoint['model']
        elif 'state_dict' in checkpoint:
            state_dict = checkpoint['state_dict']
        else:
            state_dict = checkpoint
        
        # åˆ†åˆ«åŠ è½½æ‰©æ•£æ¨¡å‹å’Œè§£ç å™¨æƒé‡
        self._load_diffusion_weights(state_dict)
        self._load_decoder_weights_improved(state_dict)
        
        print("é¢„è®­ç»ƒæƒé‡åŠ è½½å®Œæˆ")

    def _load_diffusion_weights(self, state_dict: dict) -> None:
        """åŠ è½½æ‰©æ•£æ¨¡å‹æƒé‡"""
        diffusion_dict = {}
        for key, value in state_dict.items():
            if key.startswith('diffusion.') or key.startswith('unet.'):
                diffusion_dict[key] = value
        
        if diffusion_dict:
            missing_keys, unexpected_keys = self.load_state_dict(diffusion_dict, strict=False)
            print(f"âœ… æ‰©æ•£æ¨¡å‹æƒé‡å·²åŠ è½½ ({len(diffusion_dict)} ä¸ªå‚æ•°)")
            if missing_keys:
                print(f"  ç¼ºå¤±çš„é”®: {len(missing_keys)}")
            if unexpected_keys:
                print(f"  æ„å¤–çš„é”®: {len(unexpected_keys)}")

    def _load_decoder_weights_improved(self, state_dict: dict) -> None:
        """è§£ç å™¨æƒé‡åŠ è½½ï¼ˆä»…æ”¯æŒæ–°æ ¼å¼ï¼‰"""
        # æ”¶é›†æ–°æ ¼å¼æƒé‡ï¼ˆdual_decoder.* æ ¼å¼ï¼‰
        velocity_projector_dict = {}
        velocity_decoder_dict = {}
        seismic_projector_dict = {}
        seismic_decoder_dict = {}
        
        for key, value in state_dict.items():
            if key.startswith('dual_decoder.velocity_projector.'):
                new_key = key.replace('dual_decoder.velocity_projector.', 'decoder.velocity_projector.')
                velocity_projector_dict[new_key] = value
            elif key.startswith('dual_decoder.velocity_decoder.'):
                new_key = key.replace('dual_decoder.velocity_decoder.', 'decoder.velocity_decoder.')
                velocity_decoder_dict[new_key] = value
            elif key.startswith('dual_decoder.seismic_projector.'):
                new_key = key.replace('dual_decoder.seismic_projector.', 'decoder.seismic_projector.')
                seismic_projector_dict[new_key] = value
            elif key.startswith('dual_decoder.seismic_decoder.'):
                new_key = key.replace('dual_decoder.seismic_decoder.', 'decoder.seismic_decoder.')
                seismic_decoder_dict[new_key] = value
        
        # åˆå¹¶æ‰€æœ‰æƒé‡å¹¶åŠ è½½
        all_decoder_weights = {
            **velocity_projector_dict,
            **velocity_decoder_dict,
            **seismic_projector_dict,
            **seismic_decoder_dict
        }
        
        # æŠ¥å‘ŠåŠ è½½çŠ¶æ€
        loaded_components = []
        
        if velocity_projector_dict:
            loaded_components.append(f"é€Ÿåº¦æŠ•å½±å™¨ ({len(velocity_projector_dict)} ä¸ªå‚æ•°)")
        if velocity_decoder_dict:
            loaded_components.append(f"é€Ÿåº¦è§£ç å™¨ ({len(velocity_decoder_dict)} ä¸ªå‚æ•°)")
        if seismic_projector_dict:
            loaded_components.append(f"åœ°éœ‡æŠ•å½±å™¨ ({len(seismic_projector_dict)} ä¸ªå‚æ•°)")
        if seismic_decoder_dict:
            loaded_components.append(f"åœ°éœ‡è§£ç å™¨ ({len(seismic_decoder_dict)} ä¸ªå‚æ•°)")
        
        if all_decoder_weights:
            try:
                missing_keys, unexpected_keys = self.load_state_dict(all_decoder_weights, strict=False)
                print("âœ… è§£ç å™¨æƒé‡åŠ è½½å®Œæˆ:")
                for component in loaded_components:
                    print(f"  âœ… {component}")
                
                if missing_keys:
                    print(f"  âš ï¸ ç¼ºå¤±çš„é”®: {len(missing_keys)}")
                    # æ˜¾ç¤ºå‰å‡ ä¸ªç¼ºå¤±çš„é”®
                    for key in missing_keys[:3]:
                        print(f"    - {key}")
                    if len(missing_keys) > 3:
                        print(f"    - ... è¿˜æœ‰ {len(missing_keys) - 3} ä¸ª")
                
                if unexpected_keys:
                    print(f"  âš ï¸ æ„å¤–çš„é”®: {len(unexpected_keys)}")
                    
            except Exception as e:
                print(f"âŒ è§£ç å™¨æƒé‡åŠ è½½å¤±è´¥: {e}")
                # å°è¯•é€ä¸ªç»„ä»¶åŠ è½½
                self._load_components_individually(
                    velocity_projector_dict, velocity_decoder_dict,
                    seismic_projector_dict, seismic_decoder_dict
                )
        else:
            print("âš ï¸ æœªæ‰¾åˆ°ä»»ä½•è§£ç å™¨æƒé‡è¿›è¡ŒåŠ è½½")
            print("   è¯·ç¡®ä¿æ£€æŸ¥ç‚¹æ–‡ä»¶åŒ…å« dual_decoder.* æ ¼å¼çš„å‚æ•°")

    def _load_components_individually(self, velocity_projector_dict, velocity_decoder_dict,
                                    seismic_projector_dict, seismic_decoder_dict):
        """é€ä¸ªç»„ä»¶åŠ è½½æƒé‡ï¼ˆå®¹é”™æœºåˆ¶ï¼‰"""
        print("å°è¯•é€ä¸ªç»„ä»¶åŠ è½½æƒé‡...")
        
        components = [
            ("é€Ÿåº¦æŠ•å½±å™¨", velocity_projector_dict),
            ("é€Ÿåº¦è§£ç å™¨", velocity_decoder_dict),
            ("åœ°éœ‡æŠ•å½±å™¨", seismic_projector_dict),
            ("åœ°éœ‡è§£ç å™¨", seismic_decoder_dict)
        ]
        
        for name, component_dict in components:
            if component_dict:
                try:
                    missing_keys, unexpected_keys = self.load_state_dict(component_dict, strict=False)
                    print(f"  âœ… {name}: {len(component_dict)} ä¸ªå‚æ•°")
                except Exception as e:
                    print(f"  âŒ {name} åŠ è½½å¤±è´¥: {e}")
    
    def apply_improved_quantization(self, backend: str = 'qnnpack', 
                                  convert_conv1d: bool = True,
                                  use_aggressive_config: bool = True) -> Dict[str, Any]:
        """åº”ç”¨æ”¹è¿›çš„é‡åŒ–ç­–ç•¥
        
        Args:
            backend: é‡åŒ–åç«¯
            convert_conv1d: æ˜¯å¦è½¬æ¢1Då·ç§¯
            use_aggressive_config: æ˜¯å¦ä½¿ç”¨æ¿€è¿›çš„é‡åŒ–é…ç½®
            
        Returns:
            é‡åŒ–åˆ†ææŠ¥å‘Š
        """
        print("=" * 80)
        print("ğŸ“Š åº”ç”¨æ”¹è¿›çš„é‡åŒ–ç­–ç•¥ - UB-Diffæ¨¡å‹åˆ†æ")
        print("=" * 80)
        
        # ç»Ÿè®¡æ•°æ®
        quantizable_modules = []
        non_quantizable_modules = []
        total_params = 0
        quantizable_params = 0
        
        # é‡åŒ–æ”¯æŒçš„ç±»å‹
        quantizable_types = {
            nn.Conv1d, nn.Conv2d, nn.Conv3d,
            nn.ConvTranspose1d, nn.ConvTranspose2d,
            nn.Linear,
            nn.BatchNorm1d, nn.BatchNorm2d,
            nn.ReLU, nn.ReLU6
        }
        
        # ä¸æ”¯æŒé‡åŒ–çš„ç±»å‹
        non_quantizable_types = {
            nn.LayerNorm: "LayerNormä¸æ”¯æŒé‡åŒ–",
            nn.GroupNorm: "GroupNormä¸æ”¯æŒé‡åŒ–",
            nn.GELU: "GELUæ¿€æ´»å‡½æ•°é‡åŒ–æ•ˆæœå·®",
            nn.SiLU: "SiLUæ¿€æ´»å‡½æ•°é‡åŒ–æ•ˆæœå·®", 
            nn.Tanh: "Tanhæ¿€æ´»å‡½æ•°é‡åŒ–ç²¾åº¦æŸå¤±å¤§",
            nn.Sigmoid: "Sigmoidæ¿€æ´»å‡½æ•°é‡åŒ–ç²¾åº¦æŸå¤±å¤§",
            nn.Dropout: "Dropoutå±‚æ— éœ€é‡åŒ–",
            nn.AdaptiveAvgPool1d: "è‡ªé€‚åº”æ± åŒ–ä¸æ”¯æŒé‡åŒ–",
            nn.AdaptiveAvgPool2d: "è‡ªé€‚åº”æ± åŒ–ä¸æ”¯æŒé‡åŒ–"
        }
        
        print("ğŸ” æ­£åœ¨åˆ†ææ¨¡å‹ç»“æ„...")
        
        # ç»Ÿè®¡å„ä¸ªæ¨¡å—
        from collections import defaultdict
        conv1d_count = 0
        
        for name, module in self.named_modules():
            if len(list(module.children())) == 0:  # å¶å­æ¨¡å—
                module_type = type(module)
                param_count = sum(p.numel() for p in module.parameters())
                total_params += param_count
                
                if module_type in quantizable_types:
                    quantizable_modules.append({
                        'name': name,
                        'type': module_type.__name__,
                        'params': param_count
                    })
                    quantizable_params += param_count
                    if isinstance(module, nn.Conv1d):
                        conv1d_count += 1
                elif module_type in non_quantizable_types:
                    non_quantizable_modules.append({
                        'name': name,
                        'type': module_type.__name__,
                        'params': param_count,
                        'reason': non_quantizable_types[module_type]
                    })
        
        # ç»Ÿè®¡ä¿¡æ¯
        total_modules = len(quantizable_modules) + len(non_quantizable_modules)
        quantizable_ratio = len(quantizable_modules) / total_modules if total_modules > 0 else 0
        param_ratio = quantizable_params / total_params if total_params > 0 else 0
        
        print(f"\nğŸ“ˆ é‡åŒ–åˆ†æç»“æœ:")
        print(f"  æ€»æ¨¡å—æ•°: {total_modules}")
        print(f"  æ€»å‚æ•°æ•°: {total_params:,}")
        print(f"  å¯é‡åŒ–æ¨¡å—: {len(quantizable_modules)} ({quantizable_ratio:.1%})")
        print(f"  ä¸å¯é‡åŒ–æ¨¡å—: {len(non_quantizable_modules)}")
        print(f"  å¯é‡åŒ–å‚æ•°æ¯”ä¾‹: {param_ratio:.1%}")
        
        # åˆ†ç±»ç»Ÿè®¡
        print(f"\nâœ… å¯é‡åŒ–æ¨¡å—åˆ†ç±»:")
        type_counts = defaultdict(int)
        type_params = defaultdict(int)
        
        for module in quantizable_modules:
            type_counts[module['type']] += 1
            type_params[module['type']] += module['params']
        
        for module_type, count in type_counts.items():
            params = type_params[module_type]
            print(f"  âœ“ {module_type}: {count} ä¸ªæ¨¡å—, {params:,} å‚æ•°")
        
        if conv1d_count > 0:
            print(f"  âš ï¸ å‘ç° {conv1d_count} ä¸ª1Då·ç§¯ï¼Œå»ºè®®è½¬æ¢ä¸º2Då·ç§¯")
        
        print(f"\nâŒ ä¸å¯é‡åŒ–æ¨¡å—åˆ†ç±»:")
        type_counts = defaultdict(int)
        type_params = defaultdict(int)
        
        for module in non_quantizable_modules:
            type_counts[module['type']] += 1
            type_params[module['type']] += module['params']
        
        for module_type, count in type_counts.items():
            params = type_params[module_type]
            reason = next(r for m, r in non_quantizable_types.items() if m.__name__ == module_type)
            print(f"  âœ— {module_type}: {count} ä¸ªæ¨¡å—, {params:,} å‚æ•° - {reason}")
        
        # å®é™…é‡åŒ–æ“ä½œ
        total_converted = 0
        
        # è½¬æ¢1Då·ç§¯
        if convert_conv1d and conv1d_count > 0:
            print(f"\nğŸ”„ è½¬æ¢1Då·ç§¯ä¸º2Då·ç§¯...")
            # åªè½¬æ¢æ‰©æ•£æ¨¡å‹éƒ¨åˆ†ï¼Œå› ä¸ºè§£ç å™¨å·²ç»æ˜¯é‡åŒ–çš„
            converted_diffusion = convert_conv1d_to_conv2d(self.diffusion)
            converted_unet = convert_conv1d_to_conv2d(self.unet)
            total_converted = converted_diffusion + converted_unet
            print(f"âœ“ æˆåŠŸè½¬æ¢ {total_converted} ä¸ª1Då·ç§¯å±‚")
        
        # è®¾ç½®é‡åŒ–åç«¯
        torch.backends.quantized.engine = backend
        
        # åº”ç”¨é‡åŒ–é…ç½®ï¼ˆåªåº”ç”¨ä¸€æ¬¡ï¼‰
        if self.quantize_diffusion and not hasattr(self.diffusion, 'qconfig'):
            print(f"\nâš™ï¸ é…ç½®é‡åŒ–ç­–ç•¥...")
            print(f"  é‡åŒ–åç«¯: {backend}")
            print(f"  æ¿€è¿›é…ç½®: {use_aggressive_config}")
            
            if use_aggressive_config:
                print("  ä½¿ç”¨æ¿€è¿›çš„é‡åŒ–é…ç½®...")
                # åº”ç”¨æ”¹è¿›çš„é‡åŒ–ç­–ç•¥
                self.diffusion = apply_improved_quantization(
                    self.diffusion, backend=backend, convert_conv1d=False  # å·²ç»è½¬æ¢è¿‡äº†
                )
                if hasattr(self, 'unet') and self.unet != self.diffusion:
                    self.unet = apply_improved_quantization(
                        self.unet, backend=backend, convert_conv1d=False
                    )
            else:
                print("  ä½¿ç”¨æ ‡å‡†é‡åŒ–é…ç½®...")
                qconfig = quant.get_default_qat_qconfig(backend)
                self.diffusion.qconfig = qconfig
                if hasattr(self, 'unet'):
                    self.unet.qconfig = qconfig
                    
                # å‡†å¤‡QAT
                self.train()
                quant.prepare_qat(self.diffusion, inplace=True)
                if hasattr(self, 'unet') and self.unet != self.diffusion:
                    quant.prepare_qat(self.unet, inplace=True)
        else:
            print("\nâš ï¸ æ‰©æ•£æ¨¡å‹å·²ç»é…ç½®äº†é‡åŒ–æˆ–æœªå¯ç”¨é‡åŒ–")
        
        # é‡æ–°è®¡ç®—ç»„ä»¶çº§åˆ†æï¼ˆåœ¨é‡åŒ–é…ç½®åº”ç”¨åï¼‰
        print(f"\nğŸ” ç»„ä»¶çº§é‡åŒ–èƒ½åŠ›:")
        
        components = {
            'decoder': self.decoder
        }
        
        # æ£€æŸ¥æ˜¯å¦æœ‰ç‹¬ç«‹çš„diffusionç»„ä»¶
        if hasattr(self, 'diffusion') and self.diffusion is not None:
            components['diffusion'] = self.diffusion
        
        # åªæœ‰å½“unetæ˜¯ä¸åŒçš„ç»„ä»¶æ—¶æ‰æ·»åŠ 
        if hasattr(self, 'unet') and self.unet is not None and self.unet != self.diffusion:
            components['unet'] = self.unet
        
        # é‡æ–°å®šä¹‰é‡åŒ–ç±»å‹ï¼ˆé’ˆå¯¹å¯èƒ½çš„QATé‡åŒ–æ¨¡å—ï¼‰
        qat_quantizable_types = quantizable_types.union({
            # QATåå¯èƒ½å‡ºç°çš„é‡åŒ–æ¨¡å—ç±»å‹
            quant.QuantStub,
            quant.DeQuantStub,
            type(quant.QuantStub()),
            type(quant.DeQuantStub())
        })
        
        for comp_name, component in components.items():
            comp_quantizable = 0
            comp_total = 0
            comp_params = 0
            comp_quantizable_params = 0
            
            for name, module in component.named_modules():
                if len(list(module.children())) == 0:  # å¶å­æ¨¡å—
                    comp_total += 1
                    module_params = sum(p.numel() for p in module.parameters())
                    comp_params += module_params
                    
                    # æ£€æŸ¥æ˜¯å¦ä¸ºé‡åŒ–ç±»å‹æˆ–æœ‰qconfig
                    is_quantizable = (
                        type(module) in qat_quantizable_types or
                        hasattr(module, 'qconfig') and module.qconfig is not None or
                        'quant' in type(module).__name__.lower()
                    )
                    
                    if is_quantizable:
                        comp_quantizable += 1
                        comp_quantizable_params += module_params
            
            comp_ratio = comp_quantizable / comp_total if comp_total > 0 else 0
            comp_param_ratio = comp_quantizable_params / comp_params if comp_params > 0 else 0
            
            print(f"  ğŸ“¦ {comp_name}:")
            print(f"    æ¨¡å—: {comp_quantizable}/{comp_total} ({comp_ratio:.1%}) å¯é‡åŒ–")
            print(f"    å‚æ•°: {comp_quantizable_params:,}/{comp_params:,} ({comp_param_ratio:.1%}) å¯é‡åŒ–")
        
        # é‡åŒ–å»ºè®®å’Œæ€»ç»“
        print(f"\nğŸ’¡ é‡åŒ–ä¼˜åŒ–å»ºè®®:")
        
        if conv1d_count > 0 and total_converted > 0:
            print(f"  âœ… å·²è½¬æ¢ {total_converted} ä¸ª1Då·ç§¯ä¸º2Då·ç§¯ï¼Œæå‡é‡åŒ–å…¼å®¹æ€§")
        elif conv1d_count > 0:
            print(f"  1. å»ºè®®è½¬æ¢ {conv1d_count} ä¸ª1Då·ç§¯ä¸º2Då·ç§¯ï¼Œæå‡é‡åŒ–å…¼å®¹æ€§")
        
        if param_ratio > 0.9:
            print(f"  2. æ¨¡å‹éå¸¸é€‚åˆé‡åŒ–ï¼Œé¢„æœŸå‹ç¼©æ¯”: 3-4å€")
        elif param_ratio > 0.7:
            print(f"  3. æ¨¡å‹é€‚åˆé‡åŒ–ï¼Œé¢„æœŸå‹ç¼©æ¯”: 2-3å€")
        else:
            print(f"  4. æ¨¡å‹é‡åŒ–æ”¶ç›Šæœ‰é™ï¼Œè€ƒè™‘é‡æ–°è®¾è®¡æ¶æ„")
        
        if quantizable_ratio < 0.5:
            print(f"  5. å»ºè®®å‡å°‘LayerNormå’ŒGELUç­‰ä¸å¯é‡åŒ–å±‚çš„ä½¿ç”¨")
        
        # å…³é”®å‘ç°
        print(f"\nğŸ¯ å…³é”®å‘ç°:")
        
        # LayerNormç»Ÿè®¡
        layernorm_count = sum(1 for m in non_quantizable_modules if m['type'] == 'LayerNorm')
        if layernorm_count > 0:
            layernorm_params = sum(m['params'] for m in non_quantizable_modules if m['type'] == 'LayerNorm')
            print(f"  â€¢ LayerNormå±‚: {layernorm_count} ä¸ªï¼Œ{layernorm_params:,} å‚æ•° - ä¸»è¦æ€§èƒ½ç“¶é¢ˆ")
        
        # GELUç»Ÿè®¡
        gelu_count = sum(1 for m in non_quantizable_modules if m['type'] == 'GELU')
        if gelu_count > 0:
            print(f"  â€¢ GELUæ¿€æ´»: {gelu_count} ä¸ª - å»ºè®®æ›¿æ¢ä¸ºReLUä»¥æå‡é‡åŒ–æ•ˆæœ")
        
        # SiLUç»Ÿè®¡
        silu_count = sum(1 for m in non_quantizable_modules if m['type'] == 'SiLU')
        if silu_count > 0:
            print(f"  â€¢ SiLUæ¿€æ´»: {silu_count} ä¸ª - é‡åŒ–æ”¯æŒæœ‰é™")
        
        # GroupNormç»Ÿè®¡
        groupnorm_count = sum(1 for m in non_quantizable_modules if m['type'] == 'GroupNorm')
        if groupnorm_count > 0:
            groupnorm_params = sum(m['params'] for m in non_quantizable_modules if m['type'] == 'GroupNorm')
            print(f"  â€¢ GroupNormå±‚: {groupnorm_count} ä¸ªï¼Œ{groupnorm_params:,} å‚æ•° - å»ºè®®æ›¿æ¢ä¸ºBatchNorm")
        
        print(f"\nğŸ‰ é‡åŒ–ç­–ç•¥åº”ç”¨å®Œæˆï¼")
        
        # æ„å»ºè¿”å›çš„åˆ†ææŠ¥å‘Š
        analysis_report = {
            'total_modules': total_modules,
            'total_params': total_params,
            'quantizable_modules': len(quantizable_modules),
            'quantizable_params': quantizable_params,
            'quantizable_ratio': quantizable_ratio,
            'quantizable_param_ratio': param_ratio,
            'conv1d_count': conv1d_count,
            'converted_conv1d': total_converted,
            'non_quantizable_breakdown': {
                module_type: {
                    'count': len([m for m in non_quantizable_modules if m['type'] == module_type]),
                    'params': sum(m['params'] for m in non_quantizable_modules if m['type'] == module_type)
                }
                for module_type in set(m['type'] for m in non_quantizable_modules)
            }
        }
        
        return analysis_report
    
    def prepare_for_qat_training(self, backend: str = 'qnnpack') -> None:
        """ä¸ºQATè®­ç»ƒå‡†å¤‡æ¨¡å‹"""
        print("=== å‡†å¤‡QATè®­ç»ƒ ===")
        
        # åº”ç”¨æ”¹è¿›çš„é‡åŒ–ç­–ç•¥
        self.apply_improved_quantization(
            backend=backend,
            convert_conv1d=True,
            use_aggressive_config=True
        )
        
        # è®¾ç½®è®­ç»ƒæ¨¡å¼
        self.train()
        print("âœ“ æ¨¡å‹å·²å‡†å¤‡å¥½è¿›è¡ŒQATè®­ç»ƒ")
    
    def convert_to_quantized(self) -> 'QuantizedUBDiff':
        """è½¬æ¢ä¸ºé‡åŒ–æ¨¡å‹ï¼ˆç”¨äºéƒ¨ç½²ï¼‰"""
        print("=== è½¬æ¢ä¸ºé‡åŒ–æ¨¡å‹ ===")
        
        # è®¾ç½®è¯„ä¼°æ¨¡å¼
        self.eval()
        
        # è½¬æ¢æ‰©æ•£æ¨¡å‹
        if self.quantize_diffusion:
            if hasattr(self.diffusion, 'qconfig'):
                self.diffusion = quant.convert(self.diffusion, inplace=False)
                print("âœ“ æ‰©æ•£æ¨¡å‹å·²è½¬æ¢ä¸ºé‡åŒ–ç‰ˆæœ¬")
            
            if hasattr(self.unet, 'qconfig'):
                self.unet = quant.convert(self.unet, inplace=False)
                print("âœ“ U-Netå·²è½¬æ¢ä¸ºé‡åŒ–ç‰ˆæœ¬")
        
        # è½¬æ¢è§£ç å™¨
        if self.quantize_decoder:
            if hasattr(self.decoder, 'qconfig'):
                self.decoder = quant.convert(self.decoder, inplace=False)
                print("âœ“ è§£ç å™¨å·²è½¬æ¢ä¸ºé‡åŒ–ç‰ˆæœ¬")
        
        print("ğŸ‰ é‡åŒ–è½¬æ¢å®Œæˆ")
        return self
    
    def get_model_info(self) -> Dict[str, Any]:
        """è·å–æ¨¡å‹ä¿¡æ¯"""
        total_params = sum(p.numel() for p in self.parameters())
        trainable_params = sum(p.numel() for p in self.parameters() if p.requires_grad)
        
        return {
            'total_params': total_params,
            'trainable_params': trainable_params,
            'encoder_dim': self.encoder_dim,
            'quantize_diffusion': self.quantize_diffusion,
            'quantize_decoder': self.quantize_decoder
        }


def create_quantized_model_for_deployment(checkpoint_path: str,
                                        quantize_all: bool = True) -> QuantizedUBDiff:
    """åˆ›å»ºç”¨äºéƒ¨ç½²çš„é‡åŒ–æ¨¡å‹
    
    Args:
        checkpoint_path: é¢„è®­ç»ƒæ¨¡å‹è·¯å¾„
        quantize_all: æ˜¯å¦é‡åŒ–æ‰€æœ‰ç»„ä»¶
        
    Returns:
        å‡†å¤‡éƒ¨ç½²çš„é‡åŒ–æ¨¡å‹
    """
    # åˆ›å»ºæ¨¡å‹
    model = QuantizedUBDiff(
        encoder_dim=512,
        velocity_channels=1,
        seismic_channels=5,
        dim_mults=(1, 2, 4, 8),
        time_steps=256,
        quantize_diffusion=quantize_all,
        quantize_decoder=quantize_all
    )
    
    # åŠ è½½é¢„è®­ç»ƒæƒé‡
    model.load_pretrained_weights(checkpoint_path)
    
    return model 